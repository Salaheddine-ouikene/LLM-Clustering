{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd77e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # python active_clustering.py --dataset iris --num_clusters 3 --num_seeds 10\n",
    "# python active_clustering.py --dataset 20_newsgroups_all --feature_extractor TFIDF --max-feedback-given 100 --num_clusters 20 --verbose\n",
    "# python active_clustering.py --dataset 20_newsgroups_sim3 --feature_extractor TFIDF --max-feedback-given 500 --num_clusters 3 --verbose\n",
    "# python active_clustering.py --dataset 20_newsgroups_diff3 --feature_extractor TFIDF --max-feedback-given 500 --num_clusters 3 --verbose\n",
    "\n",
    "'''\n",
    "python active_clustering.py --dataset synthetic_data \\\n",
    "    --num_clusters 5 \\\n",
    "    --num-seeds 5 \\\n",
    "    --plot-clusters \\\n",
    "    --plot-dir /tmp/synthetic_data_vanilla_kmeans_clusters\n",
    "\n",
    "python active_clustering.py --dataset OPIEC59k --data-path \\\n",
    "    /projects/ogma1/vijayv/okb-canonicalization/clustering/data \\\n",
    "    --dataset-split test \\\n",
    "    --num_clusters 490 \\\n",
    "    --num_seeds 5\n",
    "\n",
    "python active_clustering.py --dataset OPIEC59k \\\n",
    "    --data-path /projects/ogma1/vijayv/okb-canonicalization/clustering/data \\\n",
    "    --dataset-split test \\\n",
    "    --num_clusters 490 \\\n",
    "    --num_seeds 1 \\\n",
    "    --normalize-vectors \\\n",
    "    --init k-means++ \\\n",
    "    --verbose |& tee ~/logs/canon/opiec_clustering_simplified_kmeanspp.log\n",
    "'''\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "import jsonlines\n",
    "\n",
    "\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "\n",
    "\n",
    "import sys\n",
    "#sys.path.append(\"cmvc\")\n",
    "\n",
    "from dataloaders import load_dataset, generate_synthetic_data\n",
    "from experiment_utils import set_seed, summarize_results\n",
    "\n",
    "\n",
    "from active_semi_supervised_clustering.active_semi_clustering.semi_supervised.pairwise_constraints import PCKMeans, CardinalityConstrainedPCKMeans,GPTExpansionClustering, KMeansCorrection\n",
    "from active_semi_supervised_clustering.active_semi_clustering.semi_supervised.labeled_data.kmeans import KMeans\n",
    "# from sklearn.cluster import KMeans\n",
    "from active_semi_supervised_clustering.active_semi_clustering.semi_supervised.labeled_data.seededkmeans import SeededKMeans\n",
    "from active_semi_supervised_clustering.active_semi_clustering.semi_supervised.labeled_data.constrainedkmeans import ConstrainedKMeans\n",
    "from active_semi_supervised_clustering.active_semi_clustering.active.pairwise_constraints import construct_pairwise_oracle_single_example,GPT3Oracle, GPT3ComparativeOracle, DistanceBasedSelector, LabelBasedSelector, ExploreConsolidate, MinMax, SimilarityFinder, MinMaxFinetune\n",
    "from active_semi_supervised_clustering.active_semi_clustering.active.pairwise_constraints import Random\n",
    "\n",
    "from cmvc.helper import invertDic\n",
    "from cmvc.metrics import pairwiseMetric, calcF1\n",
    "from cmvc.test_performance import cluster_test\n",
    "from cmvc.model_max_margin import KGEModel\n",
    "from cmvc.Context_view import BertClassificationModel\n",
    "\n",
    "from eval_utils import cluster_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429a3a67",
   "metadata": {},
   "source": [
    "## Initialisation du parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dd433f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', type=str, choices=[\"iris\", \"tweet\", \"clinc\", \"bank77\", \"20_newsgroups_all\", \"20_newsgroups_full\", \"20_newsgroups_sim3\", \"20_newsgroups_diff3\", \"reverb45k\", \"OPIEC59k\", \"reverb45k-raw\", \"OPIEC59k-raw\", \"OPIEC59k-kg\", \"OPIEC59k-text\", \"synthetic_data\"], default=\"iris\", help=\"Clustering dataset to experiment with\")\n",
    "parser.add_argument(\"--algorithms\", action=\"append\")\n",
    "parser.add_argument('--data-path', type=str, default=None, help=\"Path to clustering data, if necessary\")\n",
    "parser.add_argument('--dataset-split', type=str, default=None, help=\"Dataset split to use, if applicable\")\n",
    "parser.add_argument('--num_clusters', type=int, default=3)\n",
    "parser.add_argument('--pckmeans-w', type=float, default=0.25, help=\"The 'w' parameter for pairwise constraint k-means\")\n",
    "parser.add_argument('--max-feedback-given', type=int, default=100, help=\"Number of instances of user feedback (e.g. oracle queries) allowed\")\n",
    "parser.add_argument('--num-corrections', type=int, default=None)\n",
    "parser.add_argument('--num_seeds', type=int, default=10)\n",
    "parser.add_argument('--num-reinit', type=int, default=1)\n",
    "parser.add_argument('--feature_extractor', type=str, choices=[\"identity\", \"BERT\", \"TFIDF\"], default=\"identity\")\n",
    "parser.add_argument('--normalize-vectors', action=\"store_true\", help=\"Normalize vectors\")\n",
    "parser.add_argument('--split-normalization', action=\"store_true\", help=\"Normalize per-view components separately (for multi-view clustering)\")\n",
    "parser.add_argument('--init', type=str, choices=[\"random\", \"k-means++\", \"k-means\"], default=\"random\", help=\"Initialization algorithm to use for k-means.\")\n",
    "parser.add_argument('--plot-clusters', action=\"store_true\", help=\"Whether to plot clusters\")\n",
    "parser.add_argument('--plot-dir', type=str, default=None, help=\"Directory to store cluster plots\")\n",
    "parser.add_argument('--verbose', action=\"store_true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06900833",
   "metadata": {},
   "source": [
    "# DÃ©finition des fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5728af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_cluster_seeds(features, labels, max_feedback_given = 0, aggregate=\"mean\"):\n",
    "    assert len(features) == len(labels)\n",
    "    labels_list = [-1 for _ in range(len(features))]\n",
    "\n",
    "    original_index_by_cluster = defaultdict(list)\n",
    "    for i, (f, l) in enumerate(zip(features, labels)):\n",
    "        original_index_by_cluster[l].append(i)\n",
    "\n",
    "    label_values = list(original_index_by_cluster.keys())\n",
    "    random.shuffle(label_values)\n",
    "\n",
    "    min_feedback_per_label = max_feedback_given // len(label_values)\n",
    "    num_labels_with_extra_point = max_feedback_given % len(label_values)\n",
    "    feedback_per_label = [min_feedback_per_label + int(i < num_labels_with_extra_point) for i in range(len(label_values))]\n",
    "\n",
    "\n",
    "    feedback_counter = 0\n",
    "    for i, label in enumerate(label_values):\n",
    "        num_feedback_for_label = min(len(original_index_by_cluster[label]), feedback_per_label[i])\n",
    "        labeled_point_indices = random.sample(original_index_by_cluster[label], num_feedback_for_label)\n",
    "        for point_index in original_index_by_cluster[label]:\n",
    "            if point_index in labeled_point_indices:\n",
    "                labels_list[point_index] = label\n",
    "                feedback_counter += 1\n",
    "\n",
    "    assert feedback_counter <= max_feedback_given\n",
    "\n",
    "    return np.array(labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc49ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_pairwise_oracle_prompt(dataset_name, documents, side_information):\n",
    "    if isinstance(side_information, list):\n",
    "        side_info = None\n",
    "    else:\n",
    "        side_info = side_information.side_info\n",
    "    if dataset_name == \"OPIEC59k\":\n",
    "        instruction = \"\"\"You are tasked with clustering entity strings based on whether they refer to the same Wikipedia article. To do this, you will be given pairs of entity names and asked if their anchor text, if used separately to link to a Wikipedia article, is likely referring to the same article. Entity names may be truncated, abbreviated, or ambiguous.\n",
    "\n",
    "To help you make this determination, you will be given up to three context sentences from Wikipedia where the entity is used as anchor text for a hyperlink. Amongst each set of examples for a given entity, the entity for all three sentences is a link to the same article on Wikipedia. Based on these examples, you will decide whether the first entity and the second entity listed would likely link to the same Wikipedia article if used as separate anchor text.\n",
    "\n",
    "Please note that the context sentences may not be representative of the entity's typical usage, but should aid in resolving the ambiguity of entities that have similar or overlapping meanings.\n",
    "\n",
    "To avoid subjective decisions, the decision should be based on a strict set of criteria, such as whether the entities will generally be used in the same contexts, whether the context sentences mention the same topic, and whether the entities have the same domain and scope of meaning.\n",
    "\n",
    "Your task will be considered successful if the entities are clustered into groups that consistently refer to the same Wikipedia articles.\"\"\"\n",
    "        example_1 = construct_pairwise_oracle_single_example(documents[side_info.ent2id[\"B.A\"]], documents[side_info.ent2id[\"M.D.\"]], \"No\", dataset_name, prompt_suffix = None, text_type = None, add_label=True)\n",
    "        example_2 = construct_pairwise_oracle_single_example(documents[side_info.ent2id[\"B.A\"]], documents[side_info.ent2id[\"bachelor\"]], \"Yes\", dataset_name, prompt_suffix = None, text_type = None, add_label=True)\n",
    "        example_3 = construct_pairwise_oracle_single_example(documents[side_info.ent2id[\"Duke of York\"]], documents[side_info.ent2id[\"Frederick\"]], \"Yes\", dataset_name, prompt_suffix = None, text_type = None, add_label=True)\n",
    "        example_4 = construct_pairwise_oracle_single_example(documents[side_info.ent2id[\"Academy Award\"]], documents[side_info.ent2id[\"Best Actor in Supporting Role\"]], \"No\", dataset_name, prompt_suffix = None, text_type = None, add_label=True)\n",
    "        prefix = \"\\n\\n\".join([example_1, example_2, example_3, example_4])\n",
    "    elif dataset_name == \"reverb45k\":\n",
    "        instruction = \"\"\"You are tasked with clustering entity strings based on whether they link to the same entity on the Freebase knowledge graph. To do this, you will be given pairs of entity names and asked if these strings, if linked to a knowledge graph, are likely referring to the same entity (e.g. a concept, person, or organization). Entity names may be truncated, abbreviated, or ambiguous.\n",
    "\n",
    "To help you make this determination, you will be given up to three context sentences from the internet that mention an entity. Amongst each set of examples for a given entity, assume that the entity mentioned in all three context sentences links refers to the same object. Based on these examples, you will decide whether the first entity and the second entity listed are likely to link to the *same* knowledge graph entity.\n",
    "\n",
    "Please note that the context sentences may not be representative of the entity's typical usage, but should aid in resolving the ambiguity of entities that have similar or overlapping meanings.\n",
    "\n",
    "To avoid subjective decisions, the decision should be based on a strict set of criteria, such as whether the entities will generally be used in the same contexts, whether the entities likely refer to the same person or organization, whether the context sentences mention the same topic, and whether the entities have the same domain and scope of meaning.\n",
    "\n",
    "Your task will be considered successful if the entities are clustered into groups that consistently link to the same knowledge graph node.\"\"\"\n",
    "        example_1 = construct_pairwise_oracle_single_example(documents[side_info.ent2id[\"Hannibal\"]], documents[side_info.ent2id[\"Hannibal Barca\"]], \"Yes\", dataset_name, prompt_suffix = None, text_type = None, add_label=True)\n",
    "        example_2 = construct_pairwise_oracle_single_example(documents[side_info.ent2id[\"Lutheran Church\"]], documents[side_info.ent2id[\"Church\"]], \"No\", dataset_name, prompt_suffix = None, text_type = None, add_label=True)\n",
    "        example_3 = construct_pairwise_oracle_single_example(documents[side_info.ent2id[\"Grove Art Online\"]], documents[side_info.ent2id[\"Oxford Art Online\"]], \"Yes\", dataset_name, prompt_suffix = None, text_type = None, add_label=True)\n",
    "        example_4 = construct_pairwise_oracle_single_example(documents[side_info.ent2id[\"Charlie Williams\"]], documents[side_info.ent2id[\"Williams\"]], \"No\", dataset_name, prompt_suffix = None, text_type = None, add_label=True)\n",
    "        prefix = \"\\n\\n\".join([example_1, example_2, example_3, example_4])\n",
    "    elif dataset_name == \"tweet\":\n",
    "        instruction = \"\"\"You are tasked with clustering tweets based on whether they discuss the same topic. To do this, you will be given pairs of (stopword-removed) tweets and asked if they discuss the same topic. To avoid subjective decisions, the decision should be based on a strict set of criteria, such as whether the tweets explicitly mention the same topic or whether they reflect the same contexts.\n",
    "\n",
    "Your task will be considered successful if the tweets are clustered into groups that consistently discuss the same topic.\"\"\"\n",
    "        example_1 = construct_pairwise_oracle_single_example(documents[0], documents[563], \"Yes\", dataset_name, prompt_suffix = None, text_type = None, add_label=True)\n",
    "        example_2 = construct_pairwise_oracle_single_example(documents[4], documents[187], \"No\", dataset_name, prompt_suffix = None, text_type = None, add_label=True)\n",
    "        example_3 = construct_pairwise_oracle_single_example(documents[2135], documents[1218], \"Yes\", dataset_name, prompt_suffix = None, text_type = None, add_label=True)\n",
    "        example_4 = construct_pairwise_oracle_single_example(documents[2471], documents[1218], \"No\", dataset_name, prompt_suffix = None, text_type = None, add_label=True)\n",
    "        prefix = \"\\n\\n\".join([example_1, example_2, example_3, example_4])\n",
    "    elif dataset_name == \"clinc\":\n",
    "        instruction = \"\"\"You are tasked with clustering queries for a task-oriented dialog system based on whether they express the same general user intent. To do this, you will be given pairs of user queries and asked if they express the same general user need or intent.\n",
    "\n",
    "Your task will be considered successful if the queries are clustered into groups that consistently express the same general intent.\"\"\"\n",
    "        example_1 = construct_pairwise_oracle_single_example(documents[1], documents[2], \"Yes\", dataset_name, prompt_suffix = None, text_type = None, add_label=True)\n",
    "        example_2 = construct_pairwise_oracle_single_example(documents[70], documents[700], \"No\", dataset_name, prompt_suffix = None, text_type = None, add_label=True)\n",
    "        example_3 = construct_pairwise_oracle_single_example(documents[1525], documents[1527], \"Yes\", dataset_name, prompt_suffix = None, text_type = None, add_label=True)\n",
    "        example_4 = construct_pairwise_oracle_single_example(documents[1500], documents[1000], \"No\", dataset_name, prompt_suffix = None, text_type = None, add_label=True)\n",
    "        prefix = \"\\n\\n\".join([example_1, example_2, example_3, example_4])\n",
    "    elif dataset_name == \"bank77\":\n",
    "        instruction = \"\"\"You are tasked with clustering queries for a online banking system based on whether they express the same general user intent. To do this, you will be given pairs of user queries and asked if they express the same general user need or intent.\n",
    "\n",
    "Your task will be considered successful if the queries are clustered into groups that consistently express the same general intent.\"\"\"\n",
    "        example_1 = construct_pairwise_oracle_single_example(documents[0], documents[1], \"Yes\", dataset_name, prompt_suffix = None, text_type = None, add_label=True)\n",
    "        example_2 = construct_pairwise_oracle_single_example(documents[1990], documents[2001], \"No\", dataset_name, prompt_suffix = None, text_type = None, add_label=True)\n",
    "        example_3 = construct_pairwise_oracle_single_example(documents[2010], documents[2001], \"Yes\", dataset_name, prompt_suffix = None, text_type = None, add_label=True)\n",
    "        example_4 = construct_pairwise_oracle_single_example(documents[2900], documents[3000], \"No\", dataset_name, prompt_suffix = None, text_type = None, add_label=True)\n",
    "        prefix = \"\\n\\n\".join([example_1, example_2, example_3, example_4])\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return \"\\n\\n\".join([instruction, prefix])\n",
    "\n",
    "def construct_keyphrase_expansion_prompt(dataset_name, documents, side_information):\n",
    "    if isinstance(side_information, list):\n",
    "        side_info = None\n",
    "    else:\n",
    "        side_info = side_information.side_info\n",
    "    if dataset_name == \"OPIEC59k\":\n",
    "        instruction = \"\"\"You are tasked with clustering entity strings based on whether they refer to the same Wikipedia article. To do this, you will be given pairs of entity names and asked if their anchor text, if used separately to link to a Wikipedia article, is likely referring to the same article. Entity names may be truncated, abbreviated, or ambiguous.\n",
    "\n",
    "To help you make this determination, you will be given up to three context sentences from Wikipedia where the entity is used as anchor text for a hyperlink. Amongst each set of examples for a given entity, the entity for all three sentences is a link to the same article on Wikipedia. Based on these examples, you will decide whether the first entity and the second entity listed would likely link to the same Wikipedia article if used as separate anchor text.\n",
    "\n",
    "Please note that the context sentences may not be representative of the entity's typical usage, but should aid in resolving the ambiguity of entities that have similar or overlapping meanings.\n",
    "\n",
    "To avoid subjective decisions, the decision should be based on a strict set of criteria, such as whether the entities will generally be used in the same contexts, whether the context sentences mention the same topic, and whether the entities have the same domain and scope of meaning.\n",
    "\n",
    "Your task will be considered successful if the entities are clustered into groups that consistently refer to the same Wikipedia articles.\"\"\"\n",
    "        example_1 = construct_pairwise_oracle_single_example(documents[side_info.ent2id[\"B.A\"]], documents[side_info.ent2id[\"M.D.\"]], \"No\", dataset_name, prompt_suffix = None, text_type = None, add_label=True)\n",
    "        example_2 = construct_pairwise_oracle_single_example(documents[side_info.ent2id[\"B.A\"]], documents[side_info.ent2id[\"bachelor\"]], \"Yes\", dataset_name, prompt_suffix = None, text_type = None, add_label=True)\n",
    "        example_3 = construct_pairwise_oracle_single_example(documents[side_info.ent2id[\"Duke of York\"]], documents[side_info.ent2id[\"Frederick\"]], \"Yes\", dataset_name, prompt_suffix = None, text_type = None, add_label=True)\n",
    "        example_4 = construct_pairwise_oracle_single_example(documents[side_info.ent2id[\"Academy Award\"]], documents[side_info.ent2id[\"Best Actor in Supporting Role\"]], \"No\", dataset_name, prompt_suffix = None, text_type = None, add_label=True)\n",
    "        prefix = \"\\n\\n\".join([example_1, example_2, example_3, example_4])\n",
    "    elif dataset_name == \"reverb45k\":\n",
    "        instruction = \"\"\"You are tasked with clustering entity strings based on whether they link to the same entity on the Freebase knowledge graph. To do this, you will be given pairs of entity names and asked if these strings, if linked to a knowledge graph, are likely referring to the same entity (e.g. a concept, person, or organization). Entity names may be truncated, abbreviated, or ambiguous.\n",
    "\n",
    "To help you make this determination, you will be given up to three context sentences from the internet that mention an entity. Amongst each set of examples for a given entity, assume that the entity mentioned in all three context sentences links refers to the same object. Based on these examples, you will decide whether the first entity and the second entity listed are likely to link to the *same* knowledge graph entity.\n",
    "\n",
    "Please note that the context sentences may not be representative of the entity's typical usage, but should aid in resolving the ambiguity of entities that have similar or overlapping meanings.\n",
    "\n",
    "To avoid subjective decisions, the decision should be based on a strict set of criteria, such as whether the entities will generally be used in the same contexts, whether the entities likely refer to the same person or organization, whether the context sentences mention the same topic, and whether the entities have the same domain and scope of meaning.\n",
    "\n",
    "Your task will be considered successful if the entities are clustered into groups that consistently link to the same knowledge graph node.\"\"\"\n",
    "        example_1 = construct_pairwise_oracle_single_example(documents[side_info.ent2id[\"Hannibal\"]], documents[side_info.ent2id[\"Hannibal Barca\"]], \"Yes\", dataset_name, prompt_suffix = None, text_type = None, add_label=True)\n",
    "        example_2 = construct_pairwise_oracle_single_example(documents[side_info.ent2id[\"Lutheran Church\"]], documents[side_info.ent2id[\"Church\"]], \"No\", dataset_name, prompt_suffix = None, text_type = None, add_label=True)\n",
    "        example_3 = construct_pairwise_oracle_single_example(documents[side_info.ent2id[\"Grove Art Online\"]], documents[side_info.ent2id[\"Oxford Art Online\"]], \"Yes\", dataset_name, prompt_suffix = None, text_type = None, add_label=True)\n",
    "        example_4 = construct_pairwise_oracle_single_example(documents[side_info.ent2id[\"Charlie Williams\"]], documents[side_info.ent2id[\"Williams\"]], \"No\", dataset_name, prompt_suffix = None, text_type = None, add_label=True)\n",
    "        prefix = \"\\n\\n\".join([example_1, example_2, example_3, example_4])\n",
    "    elif dataset_name == \"tweet\":\n",
    "        instruction = \"\"\"You are tasked with clustering tweets based on whether they discuss the same topic. To do this, you will be given pairs of (stopword-removed) tweets and asked if they discuss the same topic. To avoid subjective decisions, the decision should be based on a strict set of criteria, such as whether the tweets explicitly mention the same topic or whether they reflect the same contexts.\n",
    "\n",
    "Your task will be considered successful if the tweets are clustered into groups that consistently discuss the same topic.\"\"\"\n",
    "        example_1 = construct_pairwise_oracle_single_example(documents[0], documents[563], \"Yes\", dataset_name, prompt_suffix = None, text_type = None, add_label=True)\n",
    "        example_2 = construct_pairwise_oracle_single_example(documents[4], documents[187], \"No\", dataset_name, prompt_suffix = None, text_type = None, add_label=True)\n",
    "        example_3 = construct_pairwise_oracle_single_example(documents[2135], documents[1218], \"Yes\", dataset_name, prompt_suffix = None, text_type = None, add_label=True)\n",
    "        example_4 = construct_pairwise_oracle_single_example(documents[2471], documents[1218], \"No\", dataset_name, prompt_suffix = None, text_type = None, add_label=True)\n",
    "        prefix = \"\\n\\n\".join([example_1, example_2, example_3, example_4])\n",
    "    elif dataset_name == \"clinc\":\n",
    "        instruction = \"\"\"You are tasked with clustering queries for a task-oriented dialog system based on whether they express the same general user intent. To do this, you will be given pairs of user queries and asked if they express the same general user need or intent.\n",
    "\n",
    "Your task will be considered successful if the queries are clustered into groups that consistently express the same general intent.\"\"\"\n",
    "        example_1 = construct_pairwise_oracle_single_example(documents[1], documents[2], \"Yes\", dataset_name, prompt_suffix = None, text_type = None, add_label=True)\n",
    "        example_2 = construct_pairwise_oracle_single_example(documents[70], documents[700], \"No\", dataset_name, prompt_suffix = None, text_type = None, add_label=True)\n",
    "        example_3 = construct_pairwise_oracle_single_example(documents[1525], documents[1527], \"Yes\", dataset_name, prompt_suffix = None, text_type = None, add_label=True)\n",
    "        example_4 = construct_pairwise_oracle_single_example(documents[1500], documents[1000], \"No\", dataset_name, prompt_suffix = None, text_type = None, add_label=True)\n",
    "        prefix = \"\\n\\n\".join([example_1, example_2, example_3, example_4])\n",
    "    elif dataset_name == \"bank77\":\n",
    "        instruction = \"\"\"You are tasked with clustering queries for a online banking system based on whether they express the same general user intent. To do this, you will be given pairs of user queries and asked if they express the same general user need or intent.\n",
    "\n",
    "Your task will be considered successful if the queries are clustered into groups that consistently express the same general intent.\"\"\"\n",
    "        example_1 = construct_pairwise_oracle_single_example(documents[0], documents[1], \"Yes\", dataset_name, prompt_suffix = None, text_type = None, add_label=True)\n",
    "        example_2 = construct_pairwise_oracle_single_example(documents[1990], documents[2001], \"No\", dataset_name, prompt_suffix = None, text_type = None, add_label=True)\n",
    "        example_3 = construct_pairwise_oracle_single_example(documents[2010], documents[2001], \"Yes\", dataset_name, prompt_suffix = None, text_type = None, add_label=True)\n",
    "        example_4 = construct_pairwise_oracle_single_example(documents[2900], documents[3000], \"No\", dataset_name, prompt_suffix = None, text_type = None, add_label=True)\n",
    "        prefix = \"\\n\\n\".join([example_1, example_2, example_3, example_4])\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return \"\\n\\n\".join([instruction, prefix])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76370a92",
   "metadata": {},
   "source": [
    "## Algos de clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc38d1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(semisupervised_algo, features, documents, labels, num_clusters, dataset_name, text_type=None, prompt_suffix=None, num_corrections=100, split=None, init=\"random\", max_feedback_given=None, normalize_vectors=False, split_normalization=False, num_reinit=1, verbose=False, side_information=None, process_raw_data=False, pckmeans_w=None, seed=None):\n",
    "    pairwise_constraint_cache_name = f\"/home/gildon/Desktop/Paris_Descartes/TER/gpt4_cache/{dataset_name}_pairwise_constraint_cache.jsonl\"\n",
    "    sentence_unprocessing_mapping_file = f\"/home/gildon/Desktop/Paris_Descartes/TER/gpt4_cache/{dataset_name}_{split}_sentence_unprocessing_map.json\"\n",
    "    assert semisupervised_algo in [\"KMeans\", \"AgglomerativeClustering\", \"KMeansCorrection\", \"GPTExpansionClustering\", \"GPTExpansionAgglomerativeClustering\", \"GPTPairwiseClustering\", \"GPTPairwiseClusteringMinMax\", \"GPTPairwiseClusteringExploreSimilar\", \"GPTPairwiseClusteringOracleFree\", \"GPT_SCCL_OracleFree\", \"DEC\", \"GPT_CC_PCKMeans\", \"CardinalityConstrainedPCKMeans\", \"PCKMeans\", \"OraclePCKMeans\", \"ActivePCKMeans\", \"ActiveFinetunedPCKMeans\", \"ConstrainedKMeans\", \"SeededKMeans\"]\n",
    "    if semisupervised_algo == \"KMeans\":\n",
    "        clusterer = KMeans(n_clusters=num_clusters, normalize_vectors=normalize_vectors, split_normalization=split_normalization, init=init, num_reinit=num_reinit, verbose=verbose)\n",
    "        clusterer.fit(features)\n",
    "    elif semisupervised_algo == \"AgglomerativeClustering\":\n",
    "        clusterer = AgglomerativeClustering(n_clusters=num_clusters, linkage=\"complete\")\n",
    "        clusterer.fit(features)\n",
    "    elif semisupervised_algo == \"KMeansCorrection\":\n",
    "        labels_cache_file = f\"/home/gildon/Desktop/Paris_Descartes/TER/clustering_output/{dataset_name}_kmeans_labels.json\"\n",
    "        cluster_centers_cache_file = f\"/home/gildon/Desktop/Paris_Descartes/TER/clustering_output/{dataset_name}_kmeans_cluster_centers.npy\"\n",
    "        if os.path.exists(labels_cache_file):\n",
    "            cluster_predictions = json.load(open(labels_cache_file))\n",
    "            cluster_centers = np.load(cluster_centers_cache_file)\n",
    "        else:\n",
    "            kmeans_clusterer = KMeans(n_clusters=num_clusters, normalize_vectors=normalize_vectors, split_normalization=split_normalization, init=init, num_reinit=num_reinit, verbose=verbose)\n",
    "            kmeans_clusterer.fit(features)\n",
    "            cluster_predictions = kmeans_clusterer.labels_\n",
    "            cluster_centers = kmeans_clusterer.cluster_centers_\n",
    "            json.dump([int(l) for l in cluster_predictions], open(labels_cache_file, 'w'))\n",
    "            np.save(cluster_centers_cache_file, cluster_centers)\n",
    "\n",
    "        prompt = construct_pairwise_oracle_prompt(dataset_name, documents, side_information)\n",
    "        oracle = GPT3Oracle(features, prompt, documents, dataset_name=dataset_name, prompt_suffix=prompt_suffix, text_type=text_type, max_queries_cnt=max_feedback_given, cache_file = f\"/home/gildon/Desktop/Paris_Descartes/TER/gpt3_cache/{dataset_name}_pairwise_constraint_cache.jsonl\", model =\"gpt_4.0\")\n",
    "        clusterer = KMeansCorrection(oracle, cluster_predictions, cluster_centers, labels)\n",
    "        clusterer.fit(features, num_corrections = 100)\n",
    "\n",
    "    elif semisupervised_algo == \"GPTExpansionClustering\":\n",
    "        cache_file_name = f\"/home/gildon/Desktop/Paris_Descartes/TER/gpt4_cache/{dataset_name}_gpt_paraphrase_cache.jsonl\"\n",
    "        clusterer = GPTExpansionClustering(features, documents, algorithm=\"KMeans\", dataset_name=dataset_name, split=split, n_clusters=num_clusters, side_information=side_information, cache_file_name=cache_file_name)\n",
    "        clusterer.fit(features)\n",
    "        \n",
    "\n",
    "    elif semisupervised_algo == \"GPTPairwiseClusteringOracleFree\":\n",
    "        prompt = construct_pairwise_oracle_prompt(dataset_name, documents, side_information)\n",
    "        oracle = GPT3Oracle(features, prompt, documents, dataset_name=dataset_name, prompt_suffix=prompt_suffix, text_type=text_type, max_queries_cnt=25, cache_file = f\"/home/gildon/Desktop/Paris_Descartes/TER/gpt4_cache/{dataset_name}_pairwise_constraint_cache_reprod.jsonl\",model = \"gpt_4.0\")\n",
    "        \n",
    "        \n",
    "        print(\"Collecting Constraints\")\n",
    "        active_learner = DistanceBasedSelector(n_clusters=num_clusters)\n",
    "        active_learner.fit(features, oracle=oracle)\n",
    "        pairwise_constraints = active_learner.pairwise_constraints_\n",
    "\n",
    "        print(\"Training PCKMeans\")\n",
    "        clusterer = PCKMeans(n_clusters=num_clusters, init=init, normalize_vectors=True, split_normalization=True, side_information=side_information, w=pckmeans_w)\n",
    "        clusterer.fit(features, ml=pairwise_constraints[0], cl=pairwise_constraints[1])\n",
    "        clusterer.constraints_ = pairwise_constraints\n",
    "        if isinstance(oracle, GPT3Oracle) and os.path.exists(oracle.cache_file):\n",
    "            oracle.cache_writer.close()\n",
    "            \n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Algorithm {semisupervised_algo} not supported.\")\n",
    "    return clusterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eff8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cluster_dicts(cluster_label_list):\n",
    "    clust2ele = {}\n",
    "    for i, cluster_label in enumerate(cluster_label_list):\n",
    "        if cluster_label not in clust2ele:\n",
    "            clust2ele[cluster_label] = set()\n",
    "        clust2ele[cluster_label].add(i)\n",
    "\n",
    "    ele2clust = invertDic(clust2ele, 'm2os')\n",
    "    return ele2clust, clust2ele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6652ddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster(features, gt_labels, clusterer_labels, metrics, plot_path, pairwise_constraints=None):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    colors = [\"#e41a1c\", \"#377eb8\", \"#4daf4a\", \"#984ea3\", \"#ff7f00\"]\n",
    "    colormap = dict(zip(list(set(clusterer_labels)), colors))    \n",
    "\n",
    "    circles = []\n",
    "    min_x = 0\n",
    "    max_x = 0\n",
    "    min_y = 0\n",
    "    max_y = 0\n",
    "    for feat, gt, cluster in zip(features, gt_labels, clusterer_labels):\n",
    "        x, y = feat\n",
    "        min_x = min(min_x, x)\n",
    "        max_x = max(max_x, x)\n",
    "        min_y = min(min_y, y)\n",
    "        max_y = max(max_y, y)\n",
    "        circles.append(plt.Circle((x, y), radius=0.5, color=colormap[cluster], alpha=0.1))\n",
    "        ax.add_patch(circles[-1])\n",
    "        ax.text(x, y, str(gt), horizontalalignment='center', verticalalignment='center')\n",
    "\n",
    "    if pairwise_constraints is not None:\n",
    "        must_links, cannot_links = pairwise_constraints\n",
    "        for pml in must_links:\n",
    "            start_x_ml, start_y_ml = features[pml[0]]\n",
    "            end_x_ml, end_y_ml = features[pml[1]]\n",
    "            plt.plot(np.array([start_x_ml, end_x_ml]), np.array([start_y_ml, end_y_ml]), '-', linewidth=1, color=\"black\")\n",
    "        for pcl in cannot_links:\n",
    "            start_x_cl, start_y_cl = features[pcl[0]]\n",
    "            end_x_cl, end_y_cl = features[pcl[1]]\n",
    "            plt.plot(np.array([start_x_cl, end_x_cl]), np.array([start_y_cl, end_y_cl]), '--', linewidth=1, color=\"purple\")\n",
    "\n",
    "    ax.set_xlim((min_x-0.5, max_x+0.5))\n",
    "    ax.set_ylim((min_y-0.5, max_y+0.5))\n",
    "    nmi = metrics[\"nmi\"]\n",
    "    rand = metrics[\"rand_score\"]\n",
    "    ax.set_title(f\"Comparing ground truth clusters with true labels\\nNMI: {round(nmi, 3)}, Rand: {round(rand, 3)}\")\n",
    "    plt.legend()\n",
    "    fig.savefig(plot_path)\n",
    "    print(f\"Saved plot to {plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703560bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_algorithms(features,\n",
    "                       documents,\n",
    "                       labels,\n",
    "                       side_information,\n",
    "                       num_clusters,\n",
    "                       dataset_name,\n",
    "                       num_corrections=None,\n",
    "                       split=None,\n",
    "                       max_feedback_given=None,\n",
    "                       num_reinit=1,\n",
    "                       algorithms=[\"KMeans\", \"PCKMeans\", \"ConstrainedKMeans\", \"SeededKMeans\"],\n",
    "                       num_seeds=3,\n",
    "                       verbose=True,\n",
    "                       normalize_vectors=False,\n",
    "                       split_normalization=False,\n",
    "                       init=\"random\",\n",
    "                       plot_clusters=False,\n",
    "                       cluster_plot_dir_prefix=None,\n",
    "                       dataset=None,\n",
    "                       process_raw_data=False,\n",
    "                       pckmeans_w=None):\n",
    "    algo_results = defaultdict(list)\n",
    "    timer = time.perf_counter()\n",
    "\n",
    "    if normalize_vectors:\n",
    "        if verbose:\n",
    "            print(f\"Starting feature normalization.\")\n",
    "        if split_normalization:\n",
    "            timer = time.perf_counter()\n",
    "            kg_features = normalize(features[:, :300], axis=1, norm=\"l2\")\n",
    "            bert_features = normalize(features[:, 300:], axis=1, norm=\"l2\")\n",
    "            features = np.hstack([kg_features, bert_features])\n",
    "        else:\n",
    "            features = normalize(features, axis=1, norm=\"l2\")\n",
    "        if verbose:\n",
    "            print(f\"Feature normalization took {round(time.perf_counter() - timer, 3)} seconds.\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Starting comparison of {num_seeds} seeds:\")\n",
    "\n",
    "\n",
    "    for i, seed in enumerate(range(num_seeds)):\n",
    "\n",
    "        '''\n",
    "        if dataset == \"synthetic_data\":\n",
    "            features, labels = generate_synthetic_data(n_samples_per_cluster=200, global_seed=seed)\n",
    "            assert set(y) == set(range(len(set(y))))\n",
    "        '''\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Starting experiments for {i}th seed\")\n",
    "        set_seed(seed)\n",
    "        for semisupervised_algo in algorithms:\n",
    "            if verbose:\n",
    "                print(f\"Running {semisupervised_algo} for seed {seed}\")\n",
    "            start_time = time.perf_counter()\n",
    "            clusterer = cluster(semisupervised_algo, features, documents, labels, num_clusters, dataset_name, num_corrections=num_corrections, split=split, max_feedback_given=max_feedback_given, normalize_vectors=normalize_vectors, split_normalization=split_normalization, init=init, num_reinit=num_reinit, verbose=verbose, side_information=side_information, process_raw_data=process_raw_data, pckmeans_w=pckmeans_w, seed=seed)\n",
    "            elapsed_time = time.perf_counter() - start_time\n",
    "            if verbose:\n",
    "                print(f\"Took {round(elapsed_time, 3)} seconds to cluster points.\")\n",
    "            \n",
    "            # np.save(open(\"/projects/ogma1/vijayv/okb-canonicalization/clustering/output/OPIEC59k_test_1/OPIEC59k_clusters/kmeans/cluster_centers.npy\", 'wb'), clusterer.cluster_centers_)\n",
    "            # \n",
    "            # breakpoint()\n",
    "            metric_dict = {}\n",
    "            algo_results[semisupervised_algo].append(metric_dict)\n",
    "            if dataset_name.split('-')[0] == \"OPIEC59k\" or dataset_name.split('-')[0] == \"reverb45k\":\n",
    "                optimal_results = cluster_test(side_information.p, side_information.side_info, labels, side_information.true_ent2clust, side_information.true_clust2ent)\n",
    "                _, _, _, _, _, _, _, _, _, optimal_macro_f1, optimal_micro_f1, optimal_pairwise_f1, _, _, _, _ \\\n",
    "                    = optimal_results\n",
    "                ave_prec, ave_recall, ave_f1, macro_prec, micro_prec, pair_prec, macro_recall, micro_recall, pair_recall, macro_f1, micro_f1, pairwise_f1, model_clusters, model_Singletons, gold_clusters, gold_Singletons  = cluster_test(side_information.p, side_information.side_info, clusterer.labels_, side_information.true_ent2clust, side_information.true_clust2ent)\n",
    "\n",
    "                # Compute Macro/Macro/Pairwise F1 on OPIEC59k\n",
    "                metric_dict[\"macro_f1\"] = macro_f1\n",
    "                metric_dict[\"micro_f1\"] = micro_f1\n",
    "                metric_dict[\"pairwise_f1\"] = pairwise_f1\n",
    "                print(f\"metric_dict: {metric_dict}\")\n",
    "\n",
    "            rand_score = metrics.adjusted_rand_score(labels, clusterer.labels_)\n",
    "            metric_dict[\"rand\"] = rand_score\n",
    "            nmi = metrics.normalized_mutual_info_score(labels, clusterer.labels_)\n",
    "            metric_dict[\"nmi\"] = nmi\n",
    "\n",
    "            acc = cluster_acc(np.array(clusterer.labels_), np.array(labels))\n",
    "            metric_dict[\"acc\"] = acc\n",
    "\n",
    "            _, pred_clust2ele = generate_cluster_dicts(clusterer.labels_)\n",
    "            gt_ele2clust, gt_clust2ent = generate_cluster_dicts(labels)\n",
    "            pair_prec, pair_recall = pairwiseMetric(pred_clust2ele, gt_ele2clust, gt_clust2ent)\n",
    "            metric_dict[\"general_pairwise_f1\"] = calcF1(pair_prec, pair_recall)\n",
    "\n",
    "            if plot_clusters:\n",
    "                cluster_plot_dir = cluster_plot_dir_prefix + \"_\".join(semisupervised_algo.split())\n",
    "                os.makedirs(cluster_plot_dir, exist_ok=True)\n",
    "\n",
    "                clustering_plot_path = os.path.join(cluster_plot_dir, f\"{seed}.jpg\")\n",
    "                pcs = None if not hasattr(clusterer, \"constraints_\") else clusterer.constraints_\n",
    "                plot_cluster(features, labels, clusterer.labels_, {\"rand_score\": rand_score, \"nmi\": nmi}, clustering_plot_path, pairwise_constraints=pcs)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"\\n\")\n",
    "    return algo_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a792964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(dataset, feature_extractor, verbose=False):\n",
    "    assert feature_extractor in [\"identity\", \"BERT\", \"TFIDF\"]\n",
    "    if feature_extractor == \"identity\":\n",
    "        return dataset\n",
    "    elif feature_extractor == \"TFIDF\":\n",
    "        vectorizer = TfidfVectorizer(max_features=100000, min_df=5, encoding='latin-1', stop_words='english', lowercase=True)\n",
    "        matrix = np.array(vectorizer.fit_transform(dataset).todense())\n",
    "        if verbose:\n",
    "            print(f\"Dataset dimensions: {matrix.shape}\")\n",
    "        return matrix\n",
    "    elif feature_extractor == \"BERT\":\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b082d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv =['active_clustering.py', '--dataset', 'bank77', '--data-path', 'Desktop/Paris_Descartes/TER/Recherche_datasets/Bank77/train.csv', '--feature_extractor', 'identity', '--num_clusters', '77', '--verbose']\n",
    "\n",
    "import sys\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    algorithms=args.algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205cb4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "    X, y, documents, side_information = load_dataset(args.dataset, args.data_path, args.dataset_split, use_dse_encoder=False)\n",
    "    print(X)\n",
    "    \n",
    "    features = extract_features(X, args.feature_extractor, args.verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a1d7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "    algorithms=[\"GPTPairwiseClusteringOracleFree\"]\n",
    "    \n",
    "    process_raw_data = args.dataset.endswith(\"-raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ff9a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "    results = compare_algorithms(features,\n",
    "                                 documents,\n",
    "                                 y,\n",
    "                                 side_information,\n",
    "                                 args.num_clusters,\n",
    "                                 args.dataset,\n",
    "                                 num_corrections=args.num_corrections,\n",
    "                                 split=args.dataset_split,\n",
    "                                 max_feedback_given=args.max_feedback_given,\n",
    "                                 num_seeds=args.num_seeds,\n",
    "                                 verbose=args.verbose,\n",
    "                                 normalize_vectors=args.normalize_vectors,\n",
    "                                 split_normalization = args.split_normalization,\n",
    "                                 algorithms=algorithms,\n",
    "                                 init=args.init,\n",
    "                                 num_reinit=args.num_reinit,\n",
    "                                 plot_clusters=args.plot_clusters,\n",
    "                                 cluster_plot_dir_prefix=args.plot_dir,\n",
    "                                 dataset = args.dataset,\n",
    "                                 process_raw_data=process_raw_data,\n",
    "                                 pckmeans_w = args.pckmeans_w)\n",
    "    summarized_results = summarize_results(results)\n",
    "    print(json.dumps(summarized_results, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
