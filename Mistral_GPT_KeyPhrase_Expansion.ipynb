{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a5d9657-693c-4193-a575-2624ef3d9161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment as hungarian\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score, adjusted_rand_score, adjusted_mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "532114d5-606a-4825-8355-c902304720a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_acc(y_true, y_pred):\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "  \n",
    "    # ind = sklearn.utils.linear_assignment_.linear_assignment(w.max() - w)\n",
    "    # row_ind, col_ind = linear_assignment(w.max() - w)\n",
    "    row_ind, col_ind = hungarian(w.max() - w)\n",
    "    return sum([w[i, j] for i, j in zip(row_ind, col_ind)]) * 1.0 / y_pred.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b65c3cc-2eac-49c8-882f-06dc218644a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SubSets=[]\n",
    "\n",
    "# Load the dataset wich contains 250 articles\n",
    "data50 = pd.read_csv('bbc_news_subset_50artcl.csv')\n",
    "texts50 = data50['text'].tolist()\n",
    "SubSets.append(texts50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20bbe89b-5779-4eaf-9c7c-dbb24b25017c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "len(SubSets)\n",
    "for subset in SubSets :\n",
    "    print(len(subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38d755f4-e35e-454f-b360-188a1e634fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up OpenAI GPT and BERT\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c46edc0-73f2-4185-abd3-14c535480b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Initialize the LLaMA model\n",
    "Mistral_llm = Llama(model_path=\"./mistral-7b-v0.1.Q2_K.gguf\", verbose=False,n_ctx=2048)\n",
    "# def Llama_generate_keyphrases(text):\n",
    "   \n",
    "#     # Encode the text to tokens and truncate if necessary\n",
    "#     inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "#     tokenized_text = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "#     # Join tokens back to string if needed or use the tokenized text directly\n",
    "#     truncated_text = tokenizer.convert_tokens_to_string(tokenized_text)\n",
    "\n",
    "#     # Define the interaction with the LLaMA model\n",
    "#     response = llm.create_chat_completion(\n",
    "#         messages=[\n",
    "#             {\"role\": \"system\", \"content\": \"Generate keyphrases that describe the intent of this text.\"},\n",
    "#             {\"role\": \"user\", \"content\": truncated_text}\n",
    "#         ],\n",
    "#         max_tokens=50  # Adjust the max_tokens if needed\n",
    "#     )\n",
    "\n",
    "#     # Extracting the generated keyphrases from the response\n",
    "#     keyphrases = response['choices'][0]['message']['content'].strip()\n",
    "#     return keyphrases\n",
    "    \n",
    "def Mistral_generate_keyphrases(text):\n",
    "    # Define the interaction with the LLaMA model\n",
    "    response = Mistral_llm.create_chat_completion(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Generate keyphrases that describe the intent of this text.\"},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ]\n",
    "        #,max_tokens=50  # Adjust the max_tokens if needed\n",
    "    )\n",
    "    # Extracting the generated keyphrases from the response\n",
    "    keyphrases = response['choices'][0]['message']['content'].strip()\n",
    "    return keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe46bc92-58d0-4f16-a939-8646c215a832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].detach().numpy()  # CLS token representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3fb03dd1-3c27-4dd5-a51f-302c81fe082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization for simple clustering\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)# Try to test with others values of max_features\n",
    "\n",
    "X_simple50 = vectorizer.fit_transform(texts50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef00773e-419c-45ad-a015-273c6208dff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<50x4365 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 8052 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_simple50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3360ed83-483d-4c02-a1a8-5fa850e8c6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering\n",
    "kmeans_simple = KMeans(n_clusters=5, random_state=42)\n",
    "kmeans_enhanced = KMeans(n_clusters=5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13cd106-e57f-4034-aea0-f13a8cee5a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced vectorization using LLM keyphrases\n",
    "enhanced_vectors50 = []\n",
    "keyphrase_cache=[]\n",
    "cpt=1\n",
    "total=50\n",
    "for text in texts50:\n",
    "    keyphrase = Mistral_generate_keyphrases(text)\n",
    "    keyphrase_cache.append(keyphrase)\n",
    "    text_vector = encode_text(text)\n",
    "    keyphrase_vector = encode_text(keyphrase)\n",
    "    concatenated_vector = np.concatenate((text_vector, keyphrase_vector), axis=1)\n",
    "    enhanced_vectors50.append(concatenated_vector.squeeze())\n",
    "    print(f\"{cpt}/{total}\", end=\"\\r\")\n",
    "    cpt=cpt+1\n",
    "\n",
    "# Convert list to array\n",
    "enhanced_vectors50 = np.array(enhanced_vectors50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "705542a3-2ace-4bc6-8872-0018417421ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Mistral_enhanced_vectors50.npy', enhanced_vectors50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4e28ba2-6d9a-42a4-9201-8044645612a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ouike\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\ouike\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\ouike\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "simple_labels50 = kmeans_simple.fit_predict(X_simple50)\n",
    "enhanced_labels50 = kmeans_enhanced.fit_predict(enhanced_vectors50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63bc9815-f660-4b92-a7d3-f9293b2a4b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Clustering (50 articles) - NMI: 0.43672245047005154\n",
      "Enhanced Clustering (50 articles) - NMI: 0.6055765313391491\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (50 articles) - rand_score: 0.26850612581076905\n",
      "Enhanced Clustering (50 articles) - rand_score: 0.40334490684141033\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (50 articles) - acc: 0.6\n",
      "Enhanced Clustering (50 articles) - acc: 0.64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Evaluation with random_state=42\n",
    "nmi_simple50 = normalized_mutual_info_score(data50['category'].values, simple_labels50)\n",
    "nmi_enhanced50 = normalized_mutual_info_score(data50['category'].values, enhanced_labels50)\n",
    "print(f\"Simple Clustering (50 articles) - NMI: {nmi_simple50}\")\n",
    "print(f\"Enhanced Clustering (50 articles) - NMI: {nmi_enhanced50}\")\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "rand_score_simple50 = adjusted_rand_score(data50['category'].values, simple_labels50)\n",
    "rand_score_enhanced50 = adjusted_rand_score(data50['category'].values, enhanced_labels50)\n",
    "print(f\"Simple Clustering (50 articles) - rand_score: {rand_score_simple50}\")\n",
    "print(f\"Enhanced Clustering (50 articles) - rand_score: {rand_score_enhanced50}\")\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "# Encode category labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_true = label_encoder.fit_transform(data50['category'].values)\n",
    "\n",
    "acc_simple50 = cluster_acc(np.array(y_true), np.array(simple_labels50))\n",
    "acc_enhanced50 = cluster_acc(np.array(y_true), np.array(enhanced_labels50))\n",
    "print(f\"Simple Clustering (50 articles) - acc: {acc_simple50}\")\n",
    "print(f\"Enhanced Clustering (50 articles) - acc: {acc_enhanced50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "119cf7da-4e8f-422f-89b6-f42b2ed3f09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SubSets=[]\n",
    "\n",
    "# Load the dataset wich contains 1000 articles\n",
    "data100 = pd.read_csv('bbc_news_subset_100artcl.csv')\n",
    "texts100 = data100['text'].tolist()\n",
    "SubSets.append(texts100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c3a211d-5e0e-47c1-97b6-5b5494a256da",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_simple100 = vectorizer.fit_transform(texts100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bfd6a7-6196-4d5c-9250-b86365a9f1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/100\r"
     ]
    }
   ],
   "source": [
    "# Enhanced vectorization using LLM keyphrases\n",
    "enhanced_vectors100 = []\n",
    "keyphrase_cache=[]\n",
    "cpt=1\n",
    "total=100\n",
    "for text in texts100:\n",
    "    keyphrase = Mistral_generate_keyphrases(text)\n",
    "    keyphrase_cache.append(keyphrase)\n",
    "    text_vector = encode_text(text)\n",
    "    keyphrase_vector = encode_text(keyphrase)\n",
    "    concatenated_vector = np.concatenate((text_vector, keyphrase_vector), axis=1)\n",
    "    enhanced_vectors100.append(concatenated_vector.squeeze())\n",
    "    print(f\"{cpt}/{total}\", end=\"\\r\")\n",
    "    cpt=cpt+1\n",
    "\n",
    "# Convert list to array\n",
    "enhanced_vectors100 = np.array(enhanced_vectors100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
