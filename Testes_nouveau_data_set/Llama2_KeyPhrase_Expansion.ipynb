{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a5d9657-693c-4193-a575-2624ef3d9161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment as hungarian\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score, adjusted_rand_score, adjusted_mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "532114d5-606a-4825-8355-c902304720a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_nmi = normalized_mutual_info_score\n",
    "def cluster_acc(y_true, y_pred):\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "  \n",
    "    row_ind, col_ind = hungarian(w.max() - w)\n",
    "    return sum([w[i, j] for i, j in zip(row_ind, col_ind)]) * 1.0 / y_pred.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0b65c3cc-2eac-49c8-882f-06dc218644a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SubSets=[]\n",
    "\n",
    "# Load the dataset wich contains 250 articles\n",
    "data50 = pd.read_csv('bbc_news_subset_50artcl.csv')\n",
    "data100 = pd.read_csv('bbc_news_subset_100artcl.csv')\n",
    "texts50 = data50['text'].tolist()\n",
    "texts100 = data100['text'].tolist()\n",
    "SubSets.append(texts50)\n",
    "SubSets.append(texts100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "20bbe89b-5779-4eaf-9c7c-dbb24b25017c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "len(SubSets)\n",
    "for subset in SubSets :\n",
    "    print(len(subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38d755f4-e35e-454f-b360-188a1e634fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up OpenAI GPT and BERT\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0737f7e-7b98-404f-90bb-2e763b78c09c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7523664-d908-42e4-886d-9015d29102f0",
   "metadata": {},
   "outputs": [],
   "source": [
    " from llama_cpp import Llama\n",
    "\n",
    "# Initialize the LLaMA model\n",
    "llm = Llama(model_path=\"./llama-2-7b-chat.Q2_K.gguf\", verbose=False,n_ctx=2048)\n",
    "def Llama_generate_keyphrases(text):\n",
    "   \n",
    "    # Encode the text to tokens and truncate if necessary\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    tokenized_text = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "    # Join tokens back to string if needed or use the tokenized text directly\n",
    "    truncated_text = tokenizer.convert_tokens_to_string(tokenized_text)\n",
    "\n",
    "    # Define the interaction with the LLaMA model\n",
    "    response = llm.create_chat_completion(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Generate keyphrases that describe the intent of this text.\"},\n",
    "            {\"role\": \"user\", \"content\": truncated_text}\n",
    "        ],\n",
    "        max_tokens=50  # Adjust the max_tokens if needed\n",
    "    )\n",
    "\n",
    "    # Extracting the generated keyphrases from the response\n",
    "    keyphrases = response['choices'][0]['message']['content'].strip()\n",
    "    return keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe46bc92-58d0-4f16-a939-8646c215a832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].detach().numpy()  # CLS token representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3fb03dd1-3c27-4dd5-a51f-302c81fe082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization for simple clustering\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)# Try to test with others values of max_features\n",
    "\n",
    "X_simple50 = vectorizer.fit_transform(texts50)\n",
    "X_simple100= vectorizer.fit_transform(texts100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ef00773e-419c-45ad-a015-273c6208dff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<50x4365 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 8052 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_simple50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3360ed83-483d-4c02-a1a8-5fa850e8c6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering\n",
    "kmeans_simple = KMeans(n_clusters=5, random_state=42)\n",
    "kmeans_enhanced = KMeans(n_clusters=5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f13cd106-e57f-4034-aea0-f13a8cee5a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50\r"
     ]
    }
   ],
   "source": [
    "# Enhanced vectorization using LLM keyphrases\n",
    "enhanced_vectors50 = []\n",
    "keyphrase_cache=[]\n",
    "cpt=1\n",
    "total=50\n",
    "for text in texts50:\n",
    "    keyphrase = Llama_generate_keyphrases(text)\n",
    "    keyphrase_cache.append(keyphrase)\n",
    "    text_vector = encode_text(text)\n",
    "    keyphrase_vector = encode_text(keyphrase)\n",
    "    concatenated_vector = np.concatenate((text_vector, keyphrase_vector), axis=1)\n",
    "    enhanced_vectors50.append(concatenated_vector.squeeze())\n",
    "    print(f\"{cpt}/{total}\", end=\"\\r\")\n",
    "    cpt=cpt+1\n",
    "\n",
    "# Convert list to array\n",
    "enhanced_vectors50 = np.array(enhanced_vectors50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6b8490d8-9ec3-468d-9f2d-d2de40f7b72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Llama_enhanced_vectors50.npy', enhanced_vectors50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e45e6519-0072-4839-83eb-f55b33a249ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ouike\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\ouike\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\ouike\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "simple_labels50 = kmeans_simple.fit_predict(X_simple50)\n",
    "enhanced_labels50 = kmeans_enhanced.fit_predict(enhanced_vectors50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2892d0b5-739b-4f61-a1d3-3ed67da4409d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Clustering (50 articles) - NMI: 0.43672245047005154\n",
      "Enhanced Clustering (50 articles) - NMI: 0.5883994567867266\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (50 articles) - rand_score: 0.26850612581076905\n",
      "Enhanced Clustering (50 articles) - rand_score: 0.39307190214318755\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (50 articles) - acc: 0.6\n",
      "Enhanced Clustering (50 articles) - acc: 0.7\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Evaluation with random_state=42\n",
    "nmi_simple50 = normalized_mutual_info_score(data50['category'].values, simple_labels50)\n",
    "nmi_enhanced50 = normalized_mutual_info_score(data50['category'].values, enhanced_labels50)\n",
    "print(f\"Simple Clustering (50 articles) - NMI: {nmi_simple50}\")\n",
    "print(f\"Enhanced Clustering (50 articles) - NMI: {nmi_enhanced50}\")\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "rand_score_simple50 = adjusted_rand_score(data50['category'].values, simple_labels50)\n",
    "rand_score_enhanced50 = adjusted_rand_score(data50['category'].values, enhanced_labels50)\n",
    "print(f\"Simple Clustering (50 articles) - rand_score: {rand_score_simple50}\")\n",
    "print(f\"Enhanced Clustering (50 articles) - rand_score: {rand_score_enhanced50}\")\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "# Encode category labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_true = label_encoder.fit_transform(data50['category'].values)\n",
    "\n",
    "acc_simple50 = cluster_acc(np.array(y_true), np.array(simple_labels50))\n",
    "acc_enhanced50 = cluster_acc(np.array(y_true), np.array(enhanced_labels50))\n",
    "print(f\"Simple Clustering (50 articles) - acc: {acc_simple50}\")\n",
    "print(f\"Enhanced Clustering (50 articles) - acc: {acc_enhanced50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4e0c9d91-6915-4be3-a35c-68cdae0a4362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100\r"
     ]
    }
   ],
   "source": [
    "# Enhanced vectorization using LLM keyphrases\n",
    "enhanced_vectors100 = []\n",
    "keyphrase_cache100=[]\n",
    "cpt=1\n",
    "total=100\n",
    "for text in texts100:\n",
    "    keyphrase = Llama_generate_keyphrases(text)\n",
    "    keyphrase_cache100.append(keyphrase)\n",
    "    text_vector = encode_text(text)\n",
    "    keyphrase_vector = encode_text(keyphrase)\n",
    "    concatenated_vector = np.concatenate((text_vector, keyphrase_vector), axis=1)\n",
    "    enhanced_vectors100.append(concatenated_vector.squeeze())\n",
    "    print(f\"{cpt}/{total}\", end=\"\\r\")\n",
    "    cpt=cpt+1\n",
    "\n",
    "# Convert list to array\n",
    "enhanced_vectors100 = np.array(enhanced_vectors100)\n",
    "np.save('Llama_enhanced_vectors100.npy', enhanced_vectors100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f9a4787f-2b91-4e4e-bb8e-9c5f1ca122da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ouike\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\ouike\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\ouike\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "simple_labels100 = kmeans_simple.fit_predict(X_simple100)\n",
    "enhanced_labels100 = kmeans_enhanced.fit_predict(enhanced_vectors100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8dd852d2-8520-4332-9f8b-30f898c7703d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Clustering (100 articles) - NMI: 0.6096940422930666\n",
      "Enhanced Clustering (100 articles) - NMI: 0.8184489629920321\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (100 articles) - rand_score: 0.518511042283018\n",
      "Enhanced Clustering (100 articles) - rand_score: 0.7362427647366333\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (100 articles) - acc: 0.77\n",
      "Enhanced Clustering (100 articles) - acc: 0.86\n"
     ]
    }
   ],
   "source": [
    "# Evaluation with random_state=5\n",
    "nmi_simple100 = normalized_mutual_info_score(data100['category'].values, simple_labels100)\n",
    "nmi_enhanced100 = normalized_mutual_info_score(data100['category'].values, enhanced_labels100)\n",
    "print(f\"Simple Clustering (100 articles) - NMI: {nmi_simple100}\")\n",
    "print(f\"Enhanced Clustering (100 articles) - NMI: {nmi_enhanced100}\")\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "rand_score_simple100 = adjusted_rand_score(data100['category'].values, simple_labels100)\n",
    "rand_score_enhanced100 = adjusted_rand_score(data100['category'].values, enhanced_labels100)\n",
    "print(f\"Simple Clustering (100 articles) - rand_score: {rand_score_simple100}\")\n",
    "print(f\"Enhanced Clustering (100 articles) - rand_score: {rand_score_enhanced100}\")\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "# Encode category labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_true = label_encoder.fit_transform(data100['category'].values)\n",
    "\n",
    "acc_simple100 = cluster_acc(np.array(y_true), np.array(simple_labels100))\n",
    "acc_enhanced100 = cluster_acc(np.array(y_true), np.array(enhanced_labels100))\n",
    "print(f\"Simple Clustering (100 articles) - acc: {acc_simple100}\")\n",
    "print(f\"Enhanced Clustering (100 articles) - acc: {acc_enhanced100}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
