{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25581992-de8d-47da-8421-bd6fb27068fb",
   "metadata": {},
   "source": [
    "# Enhancing Clustering with Large Language Model \n",
    "\n",
    "This Jupyter notebook demonstrates an advanced method to enhance clustering performance using embeddings from a language model. The approach utilizes the powerful capabilities of pre-trained language models to generate key phrases and embeddings for text data, which are then used in conjunction with traditional clustering techniques.\n",
    "We implemented this code ourselves from scratch. \n",
    "## Objective\n",
    "The objective of this notebook is to compare the clustering outcomes of traditional methods with those enhanced by embeddings from a language model, specifically focusing on improvements in clustering accuracy and relevance.\n",
    "\n",
    "## Dataset\n",
    "The dataset used in this example consists of text data requiring semantic understanding for effective clustering. We use embeddings to capture deeper linguistic and semantic features that standard vectorization methods might miss.\n",
    "\n",
    "##### BBC New dataset : \n",
    "This dataset for extractive text summarization has four hundred and seventeen political news articles of BBC from 2004 to 2005 in the News Articles folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a5d9657-693c-4193-a575-2624ef3d9161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "532114d5-606a-4825-8355-c902304720a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment as hungarian\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score, adjusted_rand_score, adjusted_mutual_info_score\n",
    "\n",
    "cluster_nmi = normalized_mutual_info_score\n",
    "def cluster_acc(y_true, y_pred):\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "  \n",
    "    # ind = sklearn.utils.linear_assignment_.linear_assignment(w.max() - w)\n",
    "    # row_ind, col_ind = linear_assignment(w.max() - w)\n",
    "    row_ind, col_ind = hungarian(w.max() - w)\n",
    "    return sum([w[i, j] for i, j in zip(row_ind, col_ind)]) * 1.0 / y_pred.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b70c1d9b-0a26-43dd-99a5-bbc5bbd0b8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Load the .env file\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "if openai.api_key is None:\n",
    "    raise ValueError(\"API key is not set.\")\n",
    "else:\n",
    "    print(\"API Key loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0b65c3cc-2eac-49c8-882f-06dc218644a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SubSets=[]\n",
    "\n",
    "# Load the dataset wich contains 250 articles\n",
    "data250 = pd.read_csv('bbc_news_subset_250artcl.csv')\n",
    "texts250 = data250['text'].tolist()\n",
    "SubSets.append(texts250)\n",
    "\n",
    "# Load the dataset wich contains 500 articles\n",
    "data500 = pd.read_csv('bbc_news_subset_500artcl.csv')\n",
    "texts500 = data500['text'].tolist()\n",
    "SubSets.append(texts500)\n",
    "\n",
    "# Load the dataset wich contains 1000 articles\n",
    "data1000 = pd.read_csv('bbc_news_subset_1000artcl.csv')\n",
    "texts1000 = data1000['text'].tolist()\n",
    "SubSets.append(texts1000)\n",
    "\n",
    "# Load the dataset wich contains 1500 articles\n",
    "data1500 = pd.read_csv('bbc_news_subset_1500artcl.csv')\n",
    "texts1500 = data1500['text'].tolist()\n",
    "SubSets.append(texts1500)\n",
    "\n",
    "# Load the dataset wich contains 1850 articles\n",
    "data1850 = pd.read_csv('bbc_news_subset_1850artcl.csv')\n",
    "texts1850 = data1850['text'].tolist()\n",
    "SubSets.append(texts1850)\n",
    "\n",
    "# Load the dataset wich contains 2225 articles\n",
    "data2225 = pd.read_csv('bbc_news_full_data_2225.csv')\n",
    "texts2225 = data2225['text'].tolist()\n",
    "SubSets.append(texts2225)\n",
    "\n",
    "# Load the dataset Bank77\n",
    "dataBank77 = pd.read_csv('Bank77.csv')\n",
    "textsBank77 = dataBank77['text'].tolist()\n",
    "SubSets.append(textsBank77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "20bbe89b-5779-4eaf-9c7c-dbb24b25017c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n",
      "500\n",
      "1000\n",
      "1500\n",
      "1850\n",
      "2225\n",
      "3080\n"
     ]
    }
   ],
   "source": [
    "len(SubSets)\n",
    "for subset in SubSets :\n",
    "    print(len(subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38d755f4-e35e-454f-b360-188a1e634fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up OpenAI GPT and BERT\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33369c1-1a72-4c39-8e73-db26148806f5",
   "metadata": {},
   "source": [
    "### Key Phrase Generation using GPT-3.5 turbo\n",
    "\n",
    "We employ a language model (like GPT-3 or an alternative model if specified) to generate key phrases from the text data. These key phrases aim to capture the main themes and concepts of each text entry, which will assist in enhancing the semantic understanding of the clustering algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c46edc0-73f2-4185-abd3-14c535480b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that generate key phrases \n",
    "def generate_keyphrases(text):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Generate keyphrases that describe the intent of this text.\"},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ],\n",
    "        max_tokens=50\n",
    "    )\n",
    "    keyphrases = response.choices[0].message['content'].strip()\n",
    "    return keyphrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029f5a5c-3201-40c0-94db-54c1e4e9b074",
   "metadata": {},
   "source": [
    "### Embedding Generation\n",
    "\n",
    "After generating key phrases, we use the `DistilBert` model to create embeddings for both the original text and the generated key phrases. These embeddings represent the texts in a high-dimensional space, capturing semantic and syntactic nuances essential for effective clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe46bc92-58d0-4f16-a939-8646c215a832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].detach().numpy()  # CLS token representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3fb03dd1-3c27-4dd5-a51f-302c81fe082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization for simple clustering\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)# Try to test with others values of max_features\n",
    "\n",
    "X_simple250 = vectorizer.fit_transform(texts250)\n",
    "X_simple500 = vectorizer.fit_transform(texts500)\n",
    "X_simple1000 = vectorizer.fit_transform(texts1000)\n",
    "X_simple1500 = vectorizer.fit_transform(texts1500)\n",
    "X_simple1850 = vectorizer.fit_transform(texts1850)\n",
    "X_simple2225 = vectorizer.fit_transform(texts2225)\n",
    "X_simple2225 = vectorizer.fit_transform(texts2225)\n",
    "X_simpleBank77 = vectorizer.fit_transform(textsBank77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef00773e-419c-45ad-a015-273c6208dff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<250x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 30180 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_simple250"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c79118-2bb6-493d-a078-480dc2a2c1c3",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "This section describes the clustering process. We use the `KMeans` algorithm from scikit-learn to perform clustering on both traditionally vectorized text and the enhanced embeddings. The goal is to observe the differences in clustering performance, gauging the impact of using language model embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "3360ed83-483d-4c02-a1a8-5fa850e8c6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering\n",
    "kmeans_simple = KMeans(n_clusters=5, random_state=42)\n",
    "kmeans_enhanced = KMeans(n_clusters=5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13cd106-e57f-4034-aea0-f13a8cee5a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced vectorization using LLM keyphrases\n",
    "enhanced_vectors250 = []\n",
    "cpt=1\n",
    "for text in texts250:\n",
    "    keyphrase = generate_keyphrases(text)\n",
    "    text_vector = encode_text(text)\n",
    "    keyphrase_vector = encode_text(keyphrase)\n",
    "    concatenated_vector = np.concatenate((text_vector, keyphrase_vector), axis=1)\n",
    "    enhanced_vectors250.append(concatenated_vector.squeeze())\n",
    "    print(cpt, end=\" \")\n",
    "    cpt=cpt+1\n",
    "\n",
    "# Convert list to array\n",
    "#enhanced_vectors250 = np.array(enhanced_vectors250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "993c3b94-7d98-463f-b425-2a4b75ebdc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load your vectors back after restarting the kernel\n",
    "enhanced_vectors250 = np.load('enhanced_vectors250.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6b8490d8-9ec3-468d-9f2d-d2de40f7b72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('enhanced_vectors250.npy', enhanced_vectors250)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e45e6519-0072-4839-83eb-f55b33a249ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_labels250 = kmeans_simple.fit_predict(X_simple250)\n",
    "enhanced_labels250 = kmeans_enhanced.fit_predict(enhanced_vectors250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0398ae-98df-4b11-868e-aa26726cd516",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "To evaluate the effectiveness of our clustering approach, we calculate metrics such as Normalized Mutual Information (NMI) and the accuracy (ACC). These metrics help quantify the improvement in clustering performance due to the incorporation of language model embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2892d0b5-739b-4f61-a1d3-3ed67da4409d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Clustering (250 articles) - NMI: 0.3691888138647679\n",
      "Enhanced Clustering (250 articles) - NMI: 0.8812947638142967\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (250 articles) - rand_score: 0.26875217415654556\n",
      "Enhanced Clustering (250 articles) - rand_score: 0.8925785837313083\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (250 articles) - acc: 0.496\n",
      "Enhanced Clustering (250 articles) - acc: 0.956\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Evaluation with random_state=42\n",
    "nmi_simple250 = normalized_mutual_info_score(data250['category'].values, simple_labels250)\n",
    "nmi_enhanced250 = normalized_mutual_info_score(data250['category'].values, enhanced_labels250)\n",
    "print(f\"Simple Clustering (250 articles) - NMI: {nmi_simple250}\")\n",
    "print(f\"Enhanced Clustering (250 articles) - NMI: {nmi_enhanced250}\")\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "rand_score_simple250 = adjusted_rand_score(data250['category'].values, simple_labels250)\n",
    "rand_score_enhanced250 = adjusted_rand_score(data250['category'].values, enhanced_labels250)\n",
    "print(f\"Simple Clustering (250 articles) - rand_score: {rand_score_simple250}\")\n",
    "print(f\"Enhanced Clustering (250 articles) - rand_score: {rand_score_enhanced250}\")\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "# Encode category labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_true = label_encoder.fit_transform(data250['category'].values)\n",
    "\n",
    "acc_simple250 = cluster_acc(np.array(y_true), np.array(simple_labels250))\n",
    "acc_enhanced250 = cluster_acc(np.array(y_true), np.array(enhanced_labels250))\n",
    "print(f\"Simple Clustering (250 articles) - acc: {acc_simple250}\")\n",
    "print(f\"Enhanced Clustering (250 articles) - acc: {acc_enhanced250}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5acbf250-c425-4045-8994-d7c3af844cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Clustering (250 articles) - NMI: 0.39377379173306376\n",
      "Enhanced Clustering (250 articles) - NMI: 0.668001112361311\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (250 articles) - rand_score: 0.22410793577319602\n",
      "Enhanced Clustering (250 articles) - rand_score: 0.5856754067260372\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (250 articles) - acc: 0.544\n",
      "Enhanced Clustering (250 articles) - acc: 0.776\n"
     ]
    }
   ],
   "source": [
    "# Evaluation with random_state=25\n",
    "nmi_simple250 = normalized_mutual_info_score(data250['category'].values, simple_labels250)\n",
    "nmi_enhanced250 = normalized_mutual_info_score(data250['category'].values, enhanced_labels250)\n",
    "print(f\"Simple Clustering (250 articles) - NMI: {nmi_simple250}\")\n",
    "print(f\"Enhanced Clustering (250 articles) - NMI: {nmi_enhanced250}\")\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "rand_score_simple250 = adjusted_rand_score(data250['category'].values, simple_labels250)\n",
    "rand_score_enhanced250 = adjusted_rand_score(data250['category'].values, enhanced_labels250)\n",
    "print(f\"Simple Clustering (250 articles) - rand_score: {rand_score_simple250}\")\n",
    "print(f\"Enhanced Clustering (250 articles) - rand_score: {rand_score_enhanced250}\")\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "# Encode category labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_true = label_encoder.fit_transform(data250['category'].values)\n",
    "\n",
    "acc_simple250 = cluster_acc(np.array(y_true), np.array(simple_labels250))\n",
    "acc_enhanced250 = cluster_acc(np.array(y_true), np.array(enhanced_labels250))\n",
    "print(f\"Simple Clustering (250 articles) - acc: {acc_simple250}\")\n",
    "print(f\"Enhanced Clustering (250 articles) - acc: {acc_enhanced250}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8dd852d2-8520-4332-9f8b-30f898c7703d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Clustering (250 articles) - NMI: 0.34984114398569727\n",
      "Enhanced Clustering (250 articles) - NMI: 0.6569367998503989\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (250 articles) - rand_score: 0.29408765289664834\n",
      "Enhanced Clustering (250 articles) - rand_score: 0.5841953625342647\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (250 articles) - acc: 0.56\n",
      "Enhanced Clustering (250 articles) - acc: 0.704\n"
     ]
    }
   ],
   "source": [
    "# Evaluation with random_state=5\n",
    "nmi_simple250 = normalized_mutual_info_score(data250['category'].values, simple_labels250)\n",
    "nmi_enhanced250 = normalized_mutual_info_score(data250['category'].values, enhanced_labels250)\n",
    "print(f\"Simple Clustering (250 articles) - NMI: {nmi_simple250}\")\n",
    "print(f\"Enhanced Clustering (250 articles) - NMI: {nmi_enhanced250}\")\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "rand_score_simple250 = adjusted_rand_score(data250['category'].values, simple_labels250)\n",
    "rand_score_enhanced250 = adjusted_rand_score(data250['category'].values, enhanced_labels250)\n",
    "print(f\"Simple Clustering (250 articles) - rand_score: {rand_score_simple250}\")\n",
    "print(f\"Enhanced Clustering (250 articles) - rand_score: {rand_score_enhanced250}\")\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "# Encode category labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_true = label_encoder.fit_transform(data250['category'].values)\n",
    "\n",
    "acc_simple250 = cluster_acc(np.array(y_true), np.array(simple_labels250))\n",
    "acc_enhanced250 = cluster_acc(np.array(y_true), np.array(enhanced_labels250))\n",
    "print(f\"Simple Clustering (250 articles) - acc: {acc_simple250}\")\n",
    "print(f\"Enhanced Clustering (250 articles) - acc: {acc_enhanced250}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6da20c8f-f868-4972-a81f-ac05b69e5a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Clustering (250 articles) - NMI: 0.4812913590475506\n",
      "Enhanced Clustering (250 articles) - NMI: 0.8865459918553521\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (250 articles) - rand_score: 0.41286990104170157\n",
      "Enhanced Clustering (250 articles) - rand_score: 0.9018833290502588\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (250 articles) - acc: 0.712\n",
      "Enhanced Clustering (250 articles) - acc: 0.96\n"
     ]
    }
   ],
   "source": [
    "# Evaluation with random_state=100\n",
    "nmi_simple250 = normalized_mutual_info_score(data250['category'].values, simple_labels250)\n",
    "nmi_enhanced250 = normalized_mutual_info_score(data250['category'].values, enhanced_labels250)\n",
    "print(f\"Simple Clustering (250 articles) - NMI: {nmi_simple250}\")\n",
    "print(f\"Enhanced Clustering (250 articles) - NMI: {nmi_enhanced250}\")\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "rand_score_simple250 = adjusted_rand_score(data250['category'].values, simple_labels250)\n",
    "rand_score_enhanced250 = adjusted_rand_score(data250['category'].values, enhanced_labels250)\n",
    "print(f\"Simple Clustering (250 articles) - rand_score: {rand_score_simple250}\")\n",
    "print(f\"Enhanced Clustering (250 articles) - rand_score: {rand_score_enhanced250}\")\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "# Encode category labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_true = label_encoder.fit_transform(data250['category'].values)\n",
    "\n",
    "acc_simple250 = cluster_acc(np.array(y_true), np.array(simple_labels250))\n",
    "acc_enhanced250 = cluster_acc(np.array(y_true), np.array(enhanced_labels250))\n",
    "print(f\"Simple Clustering (250 articles) - acc: {acc_simple250}\")\n",
    "print(f\"Enhanced Clustering (250 articles) - acc: {acc_enhanced250}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78388edd-cd41-4bc4-8b08-3af120692cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced vectorization using LLM keyphrases\n",
    "enhanced_vectors500 = []\n",
    "cpt=1\n",
    "for text in texts500:\n",
    "    keyphrase = generate_keyphrases(text)\n",
    "    text_vector = encode_text(text)\n",
    "    keyphrase_vector = encode_text(keyphrase)\n",
    "    concatenated_vector = np.concatenate((text_vector, keyphrase_vector), axis=1)\n",
    "    enhanced_vectors500.append(concatenated_vector.squeeze())\n",
    "    print(cpt, end=\" \")\n",
    "    cpt=cpt+1\n",
    "\n",
    "# Convert list to array\n",
    "#enhanced_vectors500 = np.array(enhanced_vectors500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "542772d4-319a-489e-8e69-f868ae3f48d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('enhanced_vectors500.npy', enhanced_vectors500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c1f3a52-5823-43cf-8054-74db9cf3b201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load your vectors back after restarting the kernel\n",
    "enhanced_vectors500 = np.load('enhanced_vectors500.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "170927ad-f849-49af-9855-2894fce0cbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_labels500 = kmeans_simple.fit_predict(X_simple500)\n",
    "enhanced_labels500 = kmeans_enhanced.fit_predict(enhanced_vectors500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8c23dcee-3293-4383-b15a-73cf56847f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Clustering (500 articles) - NMI: 0.6591928084086461\n",
      "Enhanced Clustering (500 articles) - NMI: 0.697006647825004\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (500 articles) - rand_score: 0.5917383033198403\n",
      "Enhanced Clustering (500 articles) - rand_score: 0.585529733100578\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (500 articles) - acc: 0.806\n",
      "Enhanced Clustering (500 articles) - acc: 0.684\n"
     ]
    }
   ],
   "source": [
    "# Evaluation with random_state=42\n",
    "nmi_simple500 = normalized_mutual_info_score(data500['category'].values, simple_labels500)\n",
    "nmi_enhanced500 = normalized_mutual_info_score(data500['category'].values, enhanced_labels500)\n",
    "print(f\"Simple Clustering (500 articles) - NMI: {nmi_simple500}\")\n",
    "print(f\"Enhanced Clustering (500 articles) - NMI: {nmi_enhanced500}\")\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "rand_score_simple500 = adjusted_rand_score(data500['category'].values, simple_labels500)\n",
    "rand_score_enhanced500 = adjusted_rand_score(data500['category'].values, enhanced_labels500)\n",
    "print(f\"Simple Clustering (500 articles) - rand_score: {rand_score_simple500}\")\n",
    "print(f\"Enhanced Clustering (500 articles) - rand_score: {rand_score_enhanced500}\")\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "# Encode category labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_true = label_encoder.fit_transform(data500['category'].values)\n",
    "\n",
    "acc_simple500 = cluster_acc(np.array(y_true), np.array(simple_labels500))\n",
    "acc_enhanced500 = cluster_acc(np.array(y_true), np.array(enhanced_labels500))\n",
    "print(f\"Simple Clustering (500 articles) - acc: {acc_simple500}\")\n",
    "print(f\"Enhanced Clustering (500 articles) - acc: {acc_enhanced500}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f4dce2ea-8cbc-4ff2-90e8-1596e50e33a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Clustering (500 articles) - NMI: 0.6395706990207467\n",
      "Enhanced Clustering (500 articles) - NMI: 0.5546623961844159\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (500 articles) - rand_score: 0.5921882528094293\n",
      "Enhanced Clustering (500 articles) - rand_score: 0.49041029175700745\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (500 articles) - acc: 0.8\n",
      "Enhanced Clustering (500 articles) - acc: 0.67\n"
     ]
    }
   ],
   "source": [
    "# Evaluation with random_state=5\n",
    "nmi_simple500 = normalized_mutual_info_score(data500['category'].values, simple_labels500)\n",
    "nmi_enhanced500 = normalized_mutual_info_score(data500['category'].values, enhanced_labels500)\n",
    "print(f\"Simple Clustering (500 articles) - NMI: {nmi_simple500}\")\n",
    "print(f\"Enhanced Clustering (500 articles) - NMI: {nmi_enhanced500}\")\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "rand_score_simple500 = adjusted_rand_score(data500['category'].values, simple_labels500)\n",
    "rand_score_enhanced500 = adjusted_rand_score(data500['category'].values, enhanced_labels500)\n",
    "print(f\"Simple Clustering (500 articles) - rand_score: {rand_score_simple500}\")\n",
    "print(f\"Enhanced Clustering (500 articles) - rand_score: {rand_score_enhanced500}\")\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "# Encode category labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_true = label_encoder.fit_transform(data500['category'].values)\n",
    "\n",
    "acc_simple500 = cluster_acc(np.array(y_true), np.array(simple_labels500))\n",
    "acc_enhanced500 = cluster_acc(np.array(y_true), np.array(enhanced_labels500))\n",
    "print(f\"Simple Clustering (500 articles) - acc: {acc_simple500}\")\n",
    "print(f\"Enhanced Clustering (500 articles) - acc: {acc_enhanced500}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dddb1ac4-a1c8-497b-8f11-560d5295e759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Clustering (500 articles) - NMI: 0.3500062932703246\n",
      "Enhanced Clustering (500 articles) - NMI: 0.7375642497041326\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (500 articles) - rand_score: 0.2758815899412899\n",
      "Enhanced Clustering (500 articles) - rand_score: 0.7093300001984009\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (500 articles) - acc: 0.558\n",
      "Enhanced Clustering (500 articles) - acc: 0.866\n"
     ]
    }
   ],
   "source": [
    "# Evaluation with random_state=25\n",
    "nmi_simple500 = normalized_mutual_info_score(data500['category'].values, simple_labels500)\n",
    "nmi_enhanced500 = normalized_mutual_info_score(data500['category'].values, enhanced_labels500)\n",
    "print(f\"Simple Clustering (500 articles) - NMI: {nmi_simple500}\")\n",
    "print(f\"Enhanced Clustering (500 articles) - NMI: {nmi_enhanced500}\")\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "rand_score_simple500 = adjusted_rand_score(data500['category'].values, simple_labels500)\n",
    "rand_score_enhanced500 = adjusted_rand_score(data500['category'].values, enhanced_labels500)\n",
    "print(f\"Simple Clustering (500 articles) - rand_score: {rand_score_simple500}\")\n",
    "print(f\"Enhanced Clustering (500 articles) - rand_score: {rand_score_enhanced500}\")\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "# Encode category labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_true = label_encoder.fit_transform(data500['category'].values)\n",
    "\n",
    "acc_simple500 = cluster_acc(np.array(y_true), np.array(simple_labels500))\n",
    "acc_enhanced500 = cluster_acc(np.array(y_true), np.array(enhanced_labels500))\n",
    "print(f\"Simple Clustering (500 articles) - acc: {acc_simple500}\")\n",
    "print(f\"Enhanced Clustering (500 articles) - acc: {acc_enhanced500}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "64f35694-0724-47b3-bdb2-6d5315434ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Clustering (500 articles) - NMI: 0.3500062932703246\n",
      "Enhanced Clustering (500 articles) - NMI: 0.7375642497041326\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (500 articles) - rand_score: 0.2758815899412899\n",
      "Enhanced Clustering (500 articles) - rand_score: 0.7093300001984009\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (500 articles) - acc: 0.558\n",
      "Enhanced Clustering (500 articles) - acc: 0.866\n"
     ]
    }
   ],
   "source": [
    "# Evaluation with random_state=100\n",
    "nmi_simple500 = normalized_mutual_info_score(data500['category'].values, simple_labels500)\n",
    "nmi_enhanced500 = normalized_mutual_info_score(data500['category'].values, enhanced_labels500)\n",
    "print(f\"Simple Clustering (500 articles) - NMI: {nmi_simple500}\")\n",
    "print(f\"Enhanced Clustering (500 articles) - NMI: {nmi_enhanced500}\")\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "rand_score_simple500 = adjusted_rand_score(data500['category'].values, simple_labels500)\n",
    "rand_score_enhanced500 = adjusted_rand_score(data500['category'].values, enhanced_labels500)\n",
    "print(f\"Simple Clustering (500 articles) - rand_score: {rand_score_simple500}\")\n",
    "print(f\"Enhanced Clustering (500 articles) - rand_score: {rand_score_enhanced500}\")\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "# Encode category labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_true = label_encoder.fit_transform(data500['category'].values)\n",
    "\n",
    "acc_simple500 = cluster_acc(np.array(y_true), np.array(simple_labels500))\n",
    "acc_enhanced500 = cluster_acc(np.array(y_true), np.array(enhanced_labels500))\n",
    "print(f\"Simple Clustering (500 articles) - acc: {acc_simple500}\")\n",
    "print(f\"Enhanced Clustering (500 articles) - acc: {acc_enhanced500}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c7e56e90-c005-413a-ae6c-55796dda4d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_labels1000 = kmeans_simple.fit_predict(X_simple1000)\n",
    "enhanced_labels1000 = kmeans_enhanced.fit_predict(enhanced_vectors1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b21f65-be0a-4882-b589-05bf7ac5bcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced vectorization using LLM keyphrases\n",
    "enhanced_vectors1000 = []\n",
    "cpt=1\n",
    "for text in texts1000:\n",
    "    keyphrase = generate_keyphrases(text)\n",
    "    text_vector = encode_text(text)\n",
    "    keyphrase_vector = encode_text(keyphrase)\n",
    "    concatenated_vector = np.concatenate((text_vector, keyphrase_vector), axis=1)\n",
    "    enhanced_vectors1000.append(concatenated_vector.squeeze())\n",
    "    print(cpt, end=\" \")\n",
    "    cpt=cpt+1\n",
    "\n",
    "# Convert list to array\n",
    "enhanced_vectors1000 = np.array(enhanced_vectors1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "7786f9dc-1741-46ec-bb33-fd36daeedc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('enhanced_vectors1000.npy', enhanced_vectors1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a28d1550-d7b9-42a8-aeb9-daec9dc77e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load your vectors back after restarting the kernel\n",
    "enhanced_vectors1000 = np.load('enhanced_vectors1000.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f7614b0c-2b29-4751-b82a-a9d6513ce028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Clustering (1000 articles) - NMI: 0.7107070330249688\n",
      "Enhanced Clustering (1000 articles) - NMI: 0.7897260227199223\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (1000 articles) - rand_score: 0.5972473398593704\n",
      "Enhanced Clustering (1000 articles) - rand_score: 0.8115837105436454\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (1000 articles) - acc: 0.657\n",
      "Enhanced Clustering (1000 articles) - acc: 0.919\n"
     ]
    }
   ],
   "source": [
    "# Evaluation with random_state=42\n",
    "nmi_simple1000 = normalized_mutual_info_score(data1000['category'].values, simple_labels1000)\n",
    "nmi_enhanced1000 = normalized_mutual_info_score(data1000['category'].values, enhanced_labels1000)\n",
    "print(f\"Simple Clustering (1000 articles) - NMI: {nmi_simple1000}\")\n",
    "print(f\"Enhanced Clustering (1000 articles) - NMI: {nmi_enhanced1000}\")\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "rand_score_simple1000 = adjusted_rand_score(data1000['category'].values, simple_labels1000)\n",
    "rand_score_enhanced1000= adjusted_rand_score(data1000['category'].values, enhanced_labels1000)\n",
    "print(f\"Simple Clustering (1000 articles) - rand_score: {rand_score_simple1000}\")\n",
    "print(f\"Enhanced Clustering (1000 articles) - rand_score: {rand_score_enhanced1000}\")\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "# Encode category labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_true = label_encoder.fit_transform(data1000['category'].values)\n",
    "\n",
    "acc_simple1000 = cluster_acc(np.array(y_true), np.array(simple_labels1000))\n",
    "acc_enhanced1000 = cluster_acc(np.array(y_true), np.array(enhanced_labels1000))\n",
    "print(f\"Simple Clustering (1000 articles) - acc: {acc_simple1000}\")\n",
    "print(f\"Enhanced Clustering (1000 articles) - acc: {acc_enhanced1000}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cfbfc1a9-54b8-4777-9403-def83c534a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Clustering (1000 articles) - NMI: 0.6158248495703555\n",
      "Enhanced Clustering (1000 articles) - NMI: 0.80441556056691\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (1000 articles) - rand_score: 0.4228741170888662\n",
      "Enhanced Clustering (1000 articles) - rand_score: 0.8284102538398725\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (1000 articles) - acc: 0.605\n",
      "Enhanced Clustering (1000 articles) - acc: 0.927\n"
     ]
    }
   ],
   "source": [
    "# Evaluation with random_state=5\n",
    "nmi_simple1000 = normalized_mutual_info_score(data1000['category'].values, simple_labels1000)\n",
    "nmi_enhanced1000 = normalized_mutual_info_score(data1000['category'].values, enhanced_labels1000)\n",
    "print(f\"Simple Clustering (1000 articles) - NMI: {nmi_simple1000}\")\n",
    "print(f\"Enhanced Clustering (1000 articles) - NMI: {nmi_enhanced1000}\")\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "rand_score_simple1000 = adjusted_rand_score(data1000['category'].values, simple_labels1000)\n",
    "rand_score_enhanced1000= adjusted_rand_score(data1000['category'].values, enhanced_labels1000)\n",
    "print(f\"Simple Clustering (1000 articles) - rand_score: {rand_score_simple1000}\")\n",
    "print(f\"Enhanced Clustering (1000 articles) - rand_score: {rand_score_enhanced1000}\")\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "# Encode category labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_true = label_encoder.fit_transform(data1000['category'].values)\n",
    "\n",
    "acc_simple1000 = cluster_acc(np.array(y_true), np.array(simple_labels1000))\n",
    "acc_enhanced1000 = cluster_acc(np.array(y_true), np.array(enhanced_labels1000))\n",
    "print(f\"Simple Clustering (1000 articles) - acc: {acc_simple1000}\")\n",
    "print(f\"Enhanced Clustering (1000 articles) - acc: {acc_enhanced1000}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "73ee0e78-33a7-4ee7-ac15-9d8709a988ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Clustering (1000 articles) - NMI: 0.6306628049014542\n",
      "Enhanced Clustering (1000 articles) - NMI: 0.7150178524384878\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (1000 articles) - rand_score: 0.5470597443960421\n",
      "Enhanced Clustering (1000 articles) - rand_score: 0.6790742988737554\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (1000 articles) - acc: 0.658\n",
      "Enhanced Clustering (1000 articles) - acc: 0.844\n"
     ]
    }
   ],
   "source": [
    "# Evaluation with random_state=25\n",
    "nmi_simple1000 = normalized_mutual_info_score(data1000['category'].values, simple_labels1000)\n",
    "nmi_enhanced1000 = normalized_mutual_info_score(data1000['category'].values, enhanced_labels1000)\n",
    "print(f\"Simple Clustering (1000 articles) - NMI: {nmi_simple1000}\")\n",
    "print(f\"Enhanced Clustering (1000 articles) - NMI: {nmi_enhanced1000}\")\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "rand_score_simple1000 = adjusted_rand_score(data1000['category'].values, simple_labels1000)\n",
    "rand_score_enhanced1000= adjusted_rand_score(data1000['category'].values, enhanced_labels1000)\n",
    "print(f\"Simple Clustering (1000 articles) - rand_score: {rand_score_simple1000}\")\n",
    "print(f\"Enhanced Clustering (1000 articles) - rand_score: {rand_score_enhanced1000}\")\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "# Encode category labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_true = label_encoder.fit_transform(data1000['category'].values)\n",
    "\n",
    "acc_simple1000 = cluster_acc(np.array(y_true), np.array(simple_labels1000))\n",
    "acc_enhanced1000 = cluster_acc(np.array(y_true), np.array(enhanced_labels1000))\n",
    "print(f\"Simple Clustering (1000 articles) - acc: {acc_simple1000}\")\n",
    "print(f\"Enhanced Clustering (1000 articles) - acc: {acc_enhanced1000}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6309fbe0-106a-4a5b-8679-9169db445263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Clustering (1000 articles) - NMI: 0.5926001524246777\n",
      "Enhanced Clustering (1000 articles) - NMI: 0.7064683267253428\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (1000 articles) - rand_score: 0.46364840219395304\n",
      "Enhanced Clustering (1000 articles) - rand_score: 0.60312975762596\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (1000 articles) - acc: 0.707\n",
      "Enhanced Clustering (1000 articles) - acc: 0.678\n"
     ]
    }
   ],
   "source": [
    "# Evaluation with random_state=100\n",
    "nmi_simple1000 = normalized_mutual_info_score(data1000['category'].values, simple_labels1000)\n",
    "nmi_enhanced1000 = normalized_mutual_info_score(data1000['category'].values, enhanced_labels1000)\n",
    "print(f\"Simple Clustering (1000 articles) - NMI: {nmi_simple1000}\")\n",
    "print(f\"Enhanced Clustering (1000 articles) - NMI: {nmi_enhanced1000}\")\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "rand_score_simple1000 = adjusted_rand_score(data1000['category'].values, simple_labels1000)\n",
    "rand_score_enhanced1000= adjusted_rand_score(data1000['category'].values, enhanced_labels1000)\n",
    "print(f\"Simple Clustering (1000 articles) - rand_score: {rand_score_simple1000}\")\n",
    "print(f\"Enhanced Clustering (1000 articles) - rand_score: {rand_score_enhanced1000}\")\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "# Encode category labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_true = label_encoder.fit_transform(data1000['category'].values)\n",
    "\n",
    "acc_simple1000 = cluster_acc(np.array(y_true), np.array(simple_labels1000))\n",
    "acc_enhanced1000 = cluster_acc(np.array(y_true), np.array(enhanced_labels1000))\n",
    "print(f\"Simple Clustering (1000 articles) - acc: {acc_simple1000}\")\n",
    "print(f\"Enhanced Clustering (1000 articles) - acc: {acc_enhanced1000}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "aa973d5b-235b-4cc2-b65f-14348244f3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorization terminé\n"
     ]
    }
   ],
   "source": [
    "# Enhanced vectorization using LLM keyphrases\n",
    "enhanced_vectors1500 = []\n",
    "cpt=1\n",
    "total=1500\n",
    "for text in texts1500:\n",
    "    keyphrase = generate_keyphrases(text)\n",
    "    text_vector = encode_text(text)\n",
    "    keyphrase_vector = encode_text(keyphrase)\n",
    "    concatenated_vector = np.concatenate((text_vector, keyphrase_vector), axis=1)\n",
    "    enhanced_vectors1500.append(concatenated_vector.squeeze())\n",
    "    print(f\"{cpt}/{total}\", end=\"\\r\")\n",
    "    cpt=cpt+1\n",
    "\n",
    "# Convert list to array\n",
    "enhanced_vectors1500 = np.array(enhanced_vectors1500)\n",
    "print(\"vectorization terminé\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "fcf09865-fc3e-457f-bc9d-cf4bf33d1c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('enhanced_vectors1500.npy', enhanced_vectors1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1759fdcd-dac6-416b-8f7d-2e0c96ce9b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load your vectors back after restarting the kernel\n",
    "enhanced_vectors1500 = np.load('enhanced_vectors1500.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e1601d39-0773-4601-b2be-1df534b7ffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_labels1500 = kmeans_simple.fit_predict(X_simple1500)\n",
    "enhanced_labels1500 = kmeans_enhanced.fit_predict(enhanced_vectors1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "129967f5-4e10-4527-a670-dd76ad3fb7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Clustering (1500 articles) - NMI: 0.6698545791513273\n",
      "Enhanced Clustering (1500 articles) - NMI: 0.5585346108174444\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (1500 articles) - rand_score: 0.5577444733793943\n",
      "Enhanced Clustering (1500 articles) - rand_score: 0.49435265649272775\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (1500 articles) - acc: 0.798\n",
      "Enhanced Clustering (1500 articles) - acc: 0.6893333333333334\n"
     ]
    }
   ],
   "source": [
    "# Evaluation with random_state=42\n",
    "nmi_simple1500 = normalized_mutual_info_score(data1500['category'].values, simple_labels1500)\n",
    "nmi_enhanced1500 = normalized_mutual_info_score(data1500['category'].values, enhanced_labels1500)\n",
    "print(f\"Simple Clustering (1500 articles) - NMI: {nmi_simple1500}\")\n",
    "print(f\"Enhanced Clustering (1500 articles) - NMI: {nmi_enhanced1500}\")\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "rand_score_simple1500 = adjusted_rand_score(data1500['category'].values, simple_labels1500)\n",
    "rand_score_enhanced1500= adjusted_rand_score(data1500['category'].values, enhanced_labels1500)\n",
    "print(f\"Simple Clustering (1500 articles) - rand_score: {rand_score_simple1500}\")\n",
    "print(f\"Enhanced Clustering (1500 articles) - rand_score: {rand_score_enhanced1500}\")\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "# Encode category labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_true = label_encoder.fit_transform(data1500['category'].values)\n",
    "\n",
    "acc_simple1500 = cluster_acc(np.array(y_true), np.array(simple_labels1500))\n",
    "acc_enhanced1500 = cluster_acc(np.array(y_true), np.array(enhanced_labels1500))\n",
    "print(f\"Simple Clustering (1500 articles) - acc: {acc_simple1500}\")\n",
    "print(f\"Enhanced Clustering (1500 articles) - acc: {acc_enhanced1500}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d4367fc7-dd89-4405-bb49-22c42affebad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Clustering (1500 articles) - NMI: 0.7329658171513952\n",
      "Enhanced Clustering (1500 articles) - NMI: 0.6552321468221299\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (1500 articles) - rand_score: 0.6727914744756127\n",
      "Enhanced Clustering (1500 articles) - rand_score: 0.6081057643689253\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (1500 articles) - acc: 0.852\n",
      "Enhanced Clustering (1500 articles) - acc: 0.788\n"
     ]
    }
   ],
   "source": [
    "# Evaluation with random_state=5\n",
    "nmi_simple1500 = normalized_mutual_info_score(data1500['category'].values, simple_labels1500)\n",
    "nmi_enhanced1500 = normalized_mutual_info_score(data1500['category'].values, enhanced_labels1500)\n",
    "print(f\"Simple Clustering (1500 articles) - NMI: {nmi_simple1500}\")\n",
    "print(f\"Enhanced Clustering (1500 articles) - NMI: {nmi_enhanced1500}\")\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "rand_score_simple1500 = adjusted_rand_score(data1500['category'].values, simple_labels1500)\n",
    "rand_score_enhanced1500= adjusted_rand_score(data1500['category'].values, enhanced_labels1500)\n",
    "print(f\"Simple Clustering (1500 articles) - rand_score: {rand_score_simple1500}\")\n",
    "print(f\"Enhanced Clustering (1500 articles) - rand_score: {rand_score_enhanced1500}\")\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "# Encode category labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_true = label_encoder.fit_transform(data1500['category'].values)\n",
    "\n",
    "acc_simple1500 = cluster_acc(np.array(y_true), np.array(simple_labels1500))\n",
    "acc_enhanced1500 = cluster_acc(np.array(y_true), np.array(enhanced_labels1500))\n",
    "print(f\"Simple Clustering (1500 articles) - acc: {acc_simple1500}\")\n",
    "print(f\"Enhanced Clustering (1500 articles) - acc: {acc_enhanced1500}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "166f39a1-7a96-41cb-a251-b4e1ac5efb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Clustering (1500 articles) - NMI: 0.7651434623526583\n",
      "Enhanced Clustering (1500 articles) - NMI: 0.7431781182204052\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (1500 articles) - rand_score: 0.7676081062424845\n",
      "Enhanced Clustering (1500 articles) - rand_score: 0.72524141097845\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (1500 articles) - acc: 0.9013333333333333\n",
      "Enhanced Clustering (1500 articles) - acc: 0.8746666666666667\n"
     ]
    }
   ],
   "source": [
    "# Evaluation with random_state=25\n",
    "nmi_simple1500 = normalized_mutual_info_score(data1500['category'].values, simple_labels1500)\n",
    "nmi_enhanced1500 = normalized_mutual_info_score(data1500['category'].values, enhanced_labels1500)\n",
    "print(f\"Simple Clustering (1500 articles) - NMI: {nmi_simple1500}\")\n",
    "print(f\"Enhanced Clustering (1500 articles) - NMI: {nmi_enhanced1500}\")\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "rand_score_simple1500 = adjusted_rand_score(data1500['category'].values, simple_labels1500)\n",
    "rand_score_enhanced1500= adjusted_rand_score(data1500['category'].values, enhanced_labels1500)\n",
    "print(f\"Simple Clustering (1500 articles) - rand_score: {rand_score_simple1500}\")\n",
    "print(f\"Enhanced Clustering (1500 articles) - rand_score: {rand_score_enhanced1500}\")\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "# Encode category labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_true = label_encoder.fit_transform(data1500['category'].values)\n",
    "\n",
    "acc_simple1500 = cluster_acc(np.array(y_true), np.array(simple_labels1500))\n",
    "acc_enhanced1500 = cluster_acc(np.array(y_true), np.array(enhanced_labels1500))\n",
    "print(f\"Simple Clustering (1500 articles) - acc: {acc_simple1500}\")\n",
    "print(f\"Enhanced Clustering (1500 articles) - acc: {acc_enhanced1500}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2bd19d2c-c63f-4de0-ae15-867d7005a385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Clustering (1500 articles) - NMI: 0.5813579922501966\n",
      "Enhanced Clustering (1500 articles) - NMI: 0.8105173511771397\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (1500 articles) - rand_score: 0.3997995302586962\n",
      "Enhanced Clustering (1500 articles) - rand_score: 0.8380617259982924\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (1500 articles) - acc: 0.7393333333333333\n",
      "Enhanced Clustering (1500 articles) - acc: 0.9313333333333333\n"
     ]
    }
   ],
   "source": [
    "# Evaluation with random_state=100\n",
    "nmi_simple1500 = normalized_mutual_info_score(data1500['category'].values, simple_labels1500)\n",
    "nmi_enhanced1500 = normalized_mutual_info_score(data1500['category'].values, enhanced_labels1500)\n",
    "print(f\"Simple Clustering (1500 articles) - NMI: {nmi_simple1500}\")\n",
    "print(f\"Enhanced Clustering (1500 articles) - NMI: {nmi_enhanced1500}\")\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "rand_score_simple1500 = adjusted_rand_score(data1500['category'].values, simple_labels1500)\n",
    "rand_score_enhanced1500= adjusted_rand_score(data1500['category'].values, enhanced_labels1500)\n",
    "print(f\"Simple Clustering (1500 articles) - rand_score: {rand_score_simple1500}\")\n",
    "print(f\"Enhanced Clustering (1500 articles) - rand_score: {rand_score_enhanced1500}\")\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "# Encode category labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_true = label_encoder.fit_transform(data1500['category'].values)\n",
    "\n",
    "acc_simple1500 = cluster_acc(np.array(y_true), np.array(simple_labels1500))\n",
    "acc_enhanced1500 = cluster_acc(np.array(y_true), np.array(enhanced_labels1500))\n",
    "print(f\"Simple Clustering (1500 articles) - acc: {acc_simple1500}\")\n",
    "print(f\"Enhanced Clustering (1500 articles) - acc: {acc_enhanced1500}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "09c1b3f2-8858-4fae-9d20-92e63046935f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorization terminé avec succès\n"
     ]
    }
   ],
   "source": [
    "# Enhanced vectorization using LLM keyphrases\n",
    "enhanced_vectors1850 = []\n",
    "cpt=1\n",
    "total=1850\n",
    "for text in texts1850:\n",
    "    keyphrase = generate_keyphrases(text)\n",
    "    text_vector = encode_text(text)\n",
    "    keyphrase_vector = encode_text(keyphrase)\n",
    "    concatenated_vector = np.concatenate((text_vector, keyphrase_vector), axis=1)\n",
    "    enhanced_vectors1850.append(concatenated_vector.squeeze())\n",
    "    print(f\"{cpt}/{total}\", end=\"\\r\")\n",
    "    cpt=cpt+1\n",
    "\n",
    "# Convert list to array\n",
    "enhanced_vectors1850 = np.array(enhanced_vectors1850)\n",
    "print(\"vectorization terminé avec succès\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3c5f0ead-fca4-45f3-b303-e08f08ba0596",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('enhanced_vectors1850.npy', enhanced_vectors1850)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cb6478d0-3625-43f3-80b4-5a9717d4bde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load your vectors back after restarting the kernel\n",
    "enhanced_vectors1850 = np.load('enhanced_vectors1850.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0b4c7664-bac8-45ec-b361-c1135d5a5c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_labels1850 = kmeans_simple.fit_predict(X_simple1850)\n",
    "enhanced_labels1850 = kmeans_enhanced.fit_predict(enhanced_vectors1850)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3ff29ee0-0bb6-4767-b74f-aab7afa8226a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Clustering (1850 articles) - NMI: 0.6908902207428829\n",
      "Enhanced Clustering (1850 articles) - NMI: 0.8118052500903025\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (1850 articles) - rand_score: 0.6254892781965412\n",
      "Enhanced Clustering (1850 articles) - rand_score: 0.84652678067056\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (1850 articles) - acc: 0.82\n",
      "Enhanced Clustering (1850 articles) - acc: 0.9356756756756757\n"
     ]
    }
   ],
   "source": [
    "# Evaluation with random_state=42\n",
    "nmi_simple1850 = normalized_mutual_info_score(data1850['category'].values, simple_labels1850)\n",
    "nmi_enhanced1850 = normalized_mutual_info_score(data1850['category'].values, enhanced_labels1850)\n",
    "print(f\"Simple Clustering (1850 articles) - NMI: {nmi_simple1850}\")\n",
    "print(f\"Enhanced Clustering (1850 articles) - NMI: {nmi_enhanced1850}\")\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "rand_score_simple1850 = adjusted_rand_score(data1850['category'].values, simple_labels1850)\n",
    "rand_score_enhanced1850= adjusted_rand_score(data1850['category'].values, enhanced_labels1850)\n",
    "print(f\"Simple Clustering (1850 articles) - rand_score: {rand_score_simple1850}\")\n",
    "print(f\"Enhanced Clustering (1850 articles) - rand_score: {rand_score_enhanced1850}\")\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "# Encode category labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_true = label_encoder.fit_transform(data1850['category'].values)\n",
    "\n",
    "acc_simple1850 = cluster_acc(np.array(y_true), np.array(simple_labels1850))\n",
    "acc_enhanced1850 = cluster_acc(np.array(y_true), np.array(enhanced_labels1850))\n",
    "print(f\"Simple Clustering (1850 articles) - acc: {acc_simple1850}\")\n",
    "print(f\"Enhanced Clustering (1850 articles) - acc: {acc_enhanced1850}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f2d2521a-2d03-45f6-9ca0-3302e0674355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Clustering (1850 articles) - NMI: 0.5792921043426708\n",
      "Enhanced Clustering (1850 articles) - NMI: 0.8150372898313464\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (1850 articles) - rand_score: 0.3137601181658727\n",
      "Enhanced Clustering (1850 articles) - rand_score: 0.8500818200590934\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (1850 articles) - acc: 0.5513513513513514\n",
      "Enhanced Clustering (1850 articles) - acc: 0.9372972972972973\n"
     ]
    }
   ],
   "source": [
    "# Evaluation with random_state=5\n",
    "nmi_simple1850 = normalized_mutual_info_score(data1850['category'].values, simple_labels1850)\n",
    "nmi_enhanced1850 = normalized_mutual_info_score(data1850['category'].values, enhanced_labels1850)\n",
    "print(f\"Simple Clustering (1850 articles) - NMI: {nmi_simple1850}\")\n",
    "print(f\"Enhanced Clustering (1850 articles) - NMI: {nmi_enhanced1850}\")\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "rand_score_simple1850 = adjusted_rand_score(data1850['category'].values, simple_labels1850)\n",
    "rand_score_enhanced1850= adjusted_rand_score(data1850['category'].values, enhanced_labels1850)\n",
    "print(f\"Simple Clustering (1850 articles) - rand_score: {rand_score_simple1850}\")\n",
    "print(f\"Enhanced Clustering (1850 articles) - rand_score: {rand_score_enhanced1850}\")\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "# Encode category labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_true = label_encoder.fit_transform(data1850['category'].values)\n",
    "\n",
    "acc_simple1850 = cluster_acc(np.array(y_true), np.array(simple_labels1850))\n",
    "acc_enhanced1850 = cluster_acc(np.array(y_true), np.array(enhanced_labels1850))\n",
    "print(f\"Simple Clustering (1850 articles) - acc: {acc_simple1850}\")\n",
    "print(f\"Enhanced Clustering (1850 articles) - acc: {acc_enhanced1850}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c73ee74e-dc0a-483c-bec2-841c44021059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Clustering (1850 articles) - NMI: 0.6955787518858061\n",
      "Enhanced Clustering (1850 articles) - NMI: 0.6673058549271512\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (1850 articles) - rand_score: 0.6355990067992949\n",
      "Enhanced Clustering (1850 articles) - rand_score: 0.5828978733705864\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (1850 articles) - acc: 0.8389189189189189\n",
      "Enhanced Clustering (1850 articles) - acc: 0.6697297297297298\n"
     ]
    }
   ],
   "source": [
    "# Evaluation with random_state=25\n",
    "nmi_simple1850 = normalized_mutual_info_score(data1850['category'].values, simple_labels1850)\n",
    "nmi_enhanced1850 = normalized_mutual_info_score(data1850['category'].values, enhanced_labels1850)\n",
    "print(f\"Simple Clustering (1850 articles) - NMI: {nmi_simple1850}\")\n",
    "print(f\"Enhanced Clustering (1850 articles) - NMI: {nmi_enhanced1850}\")\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "rand_score_simple1850 = adjusted_rand_score(data1850['category'].values, simple_labels1850)\n",
    "rand_score_enhanced1850= adjusted_rand_score(data1850['category'].values, enhanced_labels1850)\n",
    "print(f\"Simple Clustering (1850 articles) - rand_score: {rand_score_simple1850}\")\n",
    "print(f\"Enhanced Clustering (1850 articles) - rand_score: {rand_score_enhanced1850}\")\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "# Encode category labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_true = label_encoder.fit_transform(data1850['category'].values)\n",
    "\n",
    "acc_simple1850 = cluster_acc(np.array(y_true), np.array(simple_labels1850))\n",
    "acc_enhanced1850 = cluster_acc(np.array(y_true), np.array(enhanced_labels1850))\n",
    "print(f\"Simple Clustering (1850 articles) - acc: {acc_simple1850}\")\n",
    "print(f\"Enhanced Clustering (1850 articles) - acc: {acc_enhanced1850}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ef155262-7d58-45f4-b601-3286c6ea3f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Clustering (1850 articles) - NMI: 0.6968951869023613\n",
      "Enhanced Clustering (1850 articles) - NMI: 0.6208853190041339\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (1850 articles) - rand_score: 0.589860579968474\n",
      "Enhanced Clustering (1850 articles) - rand_score: 0.5870128318107245\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (1850 articles) - acc: 0.7075675675675676\n",
      "Enhanced Clustering (1850 articles) - acc: 0.7248648648648649\n"
     ]
    }
   ],
   "source": [
    "# Evaluation with random_state=100\n",
    "nmi_simple1850 = normalized_mutual_info_score(data1850['category'].values, simple_labels1850)\n",
    "nmi_enhanced1850 = normalized_mutual_info_score(data1850['category'].values, enhanced_labels1850)\n",
    "print(f\"Simple Clustering (1850 articles) - NMI: {nmi_simple1850}\")\n",
    "print(f\"Enhanced Clustering (1850 articles) - NMI: {nmi_enhanced1850}\")\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "rand_score_simple1850 = adjusted_rand_score(data1850['category'].values, simple_labels1850)\n",
    "rand_score_enhanced1850= adjusted_rand_score(data1850['category'].values, enhanced_labels1850)\n",
    "print(f\"Simple Clustering (1850 articles) - rand_score: {rand_score_simple1850}\")\n",
    "print(f\"Enhanced Clustering (1850 articles) - rand_score: {rand_score_enhanced1850}\")\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "# Encode category labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_true = label_encoder.fit_transform(data1850['category'].values)\n",
    "\n",
    "acc_simple1850 = cluster_acc(np.array(y_true), np.array(simple_labels1850))\n",
    "acc_enhanced1850 = cluster_acc(np.array(y_true), np.array(enhanced_labels1850))\n",
    "print(f\"Simple Clustering (1850 articles) - acc: {acc_simple1850}\")\n",
    "print(f\"Enhanced Clustering (1850 articles) - acc: {acc_enhanced1850}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c668f9c1-afe3-4fd1-a6c7-b1bd4fa13948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorization terminé avec succès\n"
     ]
    }
   ],
   "source": [
    "# Enhanced vectorization using LLM keyphrases\n",
    "enhanced_vectors2225 = []\n",
    "cpt=1\n",
    "total=2225\n",
    "for text in texts2225:\n",
    "    keyphrase = generate_keyphrases(text)\n",
    "    text_vector = encode_text(text)\n",
    "    keyphrase_vector = encode_text(keyphrase)\n",
    "    concatenated_vector = np.concatenate((text_vector, keyphrase_vector), axis=1)\n",
    "    enhanced_vectors2225.append(concatenated_vector.squeeze())\n",
    "    print(f\"{cpt}/{total}\", end=\"\\r\")\n",
    "    cpt=cpt+1\n",
    "\n",
    "# Convert list to array\n",
    "enhanced_vectors2225 = np.array(enhanced_vectors2225)\n",
    "print(\"vectorization terminé avec succès\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "92e9bdc2-becb-40d0-bc7b-29f10a0ee757",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('enhanced_vectors2225.npy', enhanced_vectors2225)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3932dd3a-61d3-403c-83ac-d07fb2840aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load your vectors back after restarting the kernel\n",
    "enhanced_vectors2225 = np.load('enhanced_vectors2225.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "da01bd1d-56e3-4f1a-9136-3d9004d1f7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_labels2225 = kmeans_simple.fit_predict(X_simple2225)\n",
    "enhanced_labels2225 = kmeans_enhanced.fit_predict(enhanced_vectors2225)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "467654bd-756b-4368-8340-807891b74247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Clustering (2225 articles) - NMI: 0.8152174404259462\n",
      "Enhanced Clustering (2225 articles) - NMI: 0.769593084222543\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (2225 articles) - rand_score: 0.8210676146346573\n",
      "Enhanced Clustering (2225 articles) - rand_score: 0.773986984910034\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (2225 articles) - acc: 0.9267415730337079\n",
      "Enhanced Clustering (2225 articles) - acc: 0.9015730337078651\n"
     ]
    }
   ],
   "source": [
    "# Evaluation with random_state=42\n",
    "nmi_simple2225 = normalized_mutual_info_score(data2225['category'].values, simple_labels2225)\n",
    "nmi_enhanced2225 = normalized_mutual_info_score(data2225['category'].values, enhanced_labels2225)\n",
    "print(f\"Simple Clustering (2225 articles) - NMI: {nmi_simple2225}\")\n",
    "print(f\"Enhanced Clustering (2225 articles) - NMI: {nmi_enhanced2225}\")\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "rand_score_simple2225 = adjusted_rand_score(data2225['category'].values, simple_labels2225)\n",
    "rand_score_enhanced2225= adjusted_rand_score(data2225['category'].values, enhanced_labels2225)\n",
    "print(f\"Simple Clustering (2225 articles) - rand_score: {rand_score_simple2225}\")\n",
    "print(f\"Enhanced Clustering (2225 articles) - rand_score: {rand_score_enhanced2225}\")\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "# Encode category labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_true = label_encoder.fit_transform(data2225['category'].values)\n",
    "\n",
    "acc_simple2225 = cluster_acc(np.array(y_true), np.array(simple_labels2225))\n",
    "acc_enhanced2225 = cluster_acc(np.array(y_true), np.array(enhanced_labels2225))\n",
    "print(f\"Simple Clustering (2225 articles) - acc: {acc_simple2225}\")\n",
    "print(f\"Enhanced Clustering (2225 articles) - acc: {acc_enhanced2225}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "68ffbd2d-2261-42e8-8efa-d27bb41ff757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Clustering (2225 articles) - NMI: 0.6371644170801996\n",
      "Enhanced Clustering (2225 articles) - NMI: 0.8248006678446281\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (2225 articles) - rand_score: 0.4856177327497736\n",
      "Enhanced Clustering (2225 articles) - rand_score: 0.8568523219336295\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (2225 articles) - acc: 0.7330337078651685\n",
      "Enhanced Clustering (2225 articles) - acc: 0.938876404494382\n"
     ]
    }
   ],
   "source": [
    "# Evaluation with random_state=5\n",
    "nmi_simple2225 = normalized_mutual_info_score(data2225['category'].values, simple_labels2225)\n",
    "nmi_enhanced2225 = normalized_mutual_info_score(data2225['category'].values, enhanced_labels2225)\n",
    "print(f\"Simple Clustering (2225 articles) - NMI: {nmi_simple2225}\")\n",
    "print(f\"Enhanced Clustering (2225 articles) - NMI: {nmi_enhanced2225}\")\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "rand_score_simple2225 = adjusted_rand_score(data2225['category'].values, simple_labels2225)\n",
    "rand_score_enhanced2225= adjusted_rand_score(data2225['category'].values, enhanced_labels2225)\n",
    "print(f\"Simple Clustering (2225 articles) - rand_score: {rand_score_simple2225}\")\n",
    "print(f\"Enhanced Clustering (2225 articles) - rand_score: {rand_score_enhanced2225}\")\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "# Encode category labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_true = label_encoder.fit_transform(data2225['category'].values)\n",
    "\n",
    "acc_simple2225 = cluster_acc(np.array(y_true), np.array(simple_labels2225))\n",
    "acc_enhanced2225 = cluster_acc(np.array(y_true), np.array(enhanced_labels2225))\n",
    "print(f\"Simple Clustering (2225 articles) - acc: {acc_simple2225}\")\n",
    "print(f\"Enhanced Clustering (2225 articles) - acc: {acc_enhanced2225}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4bca2fd0-2a75-424b-8762-017a99a6d41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Clustering (2225 articles) - NMI: 0.6682067734759758\n",
      "Enhanced Clustering (2225 articles) - NMI: 0.6259378082157732\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (2225 articles) - rand_score: 0.5833917199785121\n",
      "Enhanced Clustering (2225 articles) - rand_score: 0.5912851227173611\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (2225 articles) - acc: 0.7047191011235955\n",
      "Enhanced Clustering (2225 articles) - acc: 0.7150561797752809\n"
     ]
    }
   ],
   "source": [
    "# Evaluation with random_state=25\n",
    "nmi_simple2225 = normalized_mutual_info_score(data2225['category'].values, simple_labels2225)\n",
    "nmi_enhanced2225 = normalized_mutual_info_score(data2225['category'].values, enhanced_labels2225)\n",
    "print(f\"Simple Clustering (2225 articles) - NMI: {nmi_simple2225}\")\n",
    "print(f\"Enhanced Clustering (2225 articles) - NMI: {nmi_enhanced2225}\")\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "rand_score_simple2225 = adjusted_rand_score(data2225['category'].values, simple_labels2225)\n",
    "rand_score_enhanced2225= adjusted_rand_score(data2225['category'].values, enhanced_labels2225)\n",
    "print(f\"Simple Clustering (2225 articles) - rand_score: {rand_score_simple2225}\")\n",
    "print(f\"Enhanced Clustering (2225 articles) - rand_score: {rand_score_enhanced2225}\")\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "# Encode category labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_true = label_encoder.fit_transform(data2225['category'].values)\n",
    "\n",
    "acc_simple2225 = cluster_acc(np.array(y_true), np.array(simple_labels2225))\n",
    "acc_enhanced2225 = cluster_acc(np.array(y_true), np.array(enhanced_labels2225))\n",
    "print(f\"Simple Clustering (2225 articles) - acc: {acc_simple2225}\")\n",
    "print(f\"Enhanced Clustering (2225 articles) - acc: {acc_enhanced2225}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f0ef9057-9946-4721-9825-17aa86242b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Clustering (2225 articles) - NMI: 0.6971609909162098\n",
      "Enhanced Clustering (2225 articles) - NMI: 0.7729247182727357\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (2225 articles) - rand_score: 0.6617560796405119\n",
      "Enhanced Clustering (2225 articles) - rand_score: 0.7764295234622282\n",
      "---------------------------------------------------------------------------------\n",
      "Simple Clustering (2225 articles) - acc: 0.8058426966292135\n",
      "Enhanced Clustering (2225 articles) - acc: 0.9029213483146067\n"
     ]
    }
   ],
   "source": [
    "# Evaluation with random_state=100\n",
    "nmi_simple2225 = normalized_mutual_info_score(data2225['category'].values, simple_labels2225)\n",
    "nmi_enhanced2225 = normalized_mutual_info_score(data2225['category'].values, enhanced_labels2225)\n",
    "print(f\"Simple Clustering (2225 articles) - NMI: {nmi_simple2225}\")\n",
    "print(f\"Enhanced Clustering (2225 articles) - NMI: {nmi_enhanced2225}\")\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "rand_score_simple2225 = adjusted_rand_score(data2225['category'].values, simple_labels2225)\n",
    "rand_score_enhanced2225= adjusted_rand_score(data2225['category'].values, enhanced_labels2225)\n",
    "print(f\"Simple Clustering (2225 articles) - rand_score: {rand_score_simple2225}\")\n",
    "print(f\"Enhanced Clustering (2225 articles) - rand_score: {rand_score_enhanced2225}\")\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "# Encode category labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_true = label_encoder.fit_transform(data2225['category'].values)\n",
    "\n",
    "acc_simple2225 = cluster_acc(np.array(y_true), np.array(simple_labels2225))\n",
    "acc_enhanced2225 = cluster_acc(np.array(y_true), np.array(enhanced_labels2225))\n",
    "print(f\"Simple Clustering (2225 articles) - acc: {acc_simple2225}\")\n",
    "print(f\"Enhanced Clustering (2225 articles) - acc: {acc_enhanced2225}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb2f2b9-6860-401e-97a0-adb05e1fbca9",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We conclude by summarizing the findings from our clustering experiments. The results are discussed in the context of how language model embeddings have potentially improved the clustering outcomes and what this might mean for practical applications of clustering in natural language processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c476d334-ca80-4936-bb28-dd2371615470",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
